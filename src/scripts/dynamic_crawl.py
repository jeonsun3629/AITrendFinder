import asyncio
import json
import os
import sys
import time
import re
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional, Union
import argparse
import urllib.parse # URL ì •ê·œí™”ë¥¼ ìœ„í•´ ì¶”ê°€

# Windowsì—ì„œ ìœ ë‹ˆì½”ë“œ ì¶œë ¥ì„ ìœ„í•œ ì„¤ì •
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë™ì  ì„¤ì¹˜
def ensure_installed(package: str) -> None:
    try:
        __import__(package)
    except ImportError:
        import subprocess
        print(f"Installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
ensure_installed("playwright")
ensure_installed("beautifulsoup4")
ensure_installed("openai")
ensure_installed("python-dateutil")

# ì„¤ì¹˜ í›„ ì„í¬íŠ¸
from playwright.async_api import async_playwright, Page, Browser, BrowserContext
from bs4 import BeautifulSoup
import openai
from dateutil import parser as date_parser

# ì‚¬ì´íŠ¸ ì„¤ì • ì‹œìŠ¤í…œ ì„í¬íŠ¸
# from site_configs import site_config_manager, SiteConfig

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.environ.get("OPENAI_API_KEY", "")

# JSON ì„¤ì • ë¡œë”
class JSONConfigLoader:
    """JSON íŒŒì¼ì—ì„œ ì‚¬ì´íŠ¸ ì„¤ì •ì„ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    @staticmethod
    def load_config_from_json(json_path: str = "site_configs.json"):
        """JSON íŒŒì¼ì—ì„œ ì‚¬ì´íŠ¸ ì„¤ì •ì„ ë¡œë“œí•˜ì—¬ site_config_managerì— ì¶”ê°€"""
        print("âš ï¸ ê°„ì†Œí™”ëœ ë²„ì „: JSON ì„¤ì • ë¡œë” ë¹„í™œì„±í™”ë¨")
        return

# í–¥ìƒëœ ì‚¬ì´íŠ¸ ê°ì§€ í´ë˜ìŠ¤
class SiteDetector:
    """ì›¹ì‚¬ì´íŠ¸ì˜ ìœ í˜•ê³¼ êµ¬ì¡°ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ëŠ” í´ë˜ìŠ¤"""
    
    @staticmethod
    async def detect_site_type(page: Page) -> Dict[str, Any]:
        """í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ì´íŠ¸ ìœ í˜•ê³¼ íŠ¹ì„±ì„ ê°ì§€"""
        try:
            detection_result = await page.evaluate(r'''() => {
                const result = {
                    platform: 'unknown',
                    cms: 'unknown',
                    structure: 'unknown',
                    features: [],
                    meta_info: {},
                    selectors: {
                        articles: [],
                        content: [],
                        dates: []
                    }
                };
                
                // HTMLê³¼ head ì •ë³´ ë¶„ì„
                const html = document.documentElement.outerHTML.toLowerCase();
                const headContent = document.head ? document.head.innerHTML.toLowerCase() : '';
                
                // CMS ê°ì§€
                if (html.includes('wp-content') || html.includes('wordpress') || html.includes('/wp-includes/')) {
                    result.cms = 'wordpress';
                    result.platform = 'blog';
                } else if (html.includes('medium-feed') || window.location.hostname.includes('medium')) {
                    result.cms = 'medium';
                    result.platform = 'blog';
                } else if (html.includes('reddit') || window.location.hostname.includes('reddit')) {
                    result.cms = 'reddit';
                    result.platform = 'social';
                } else if (html.includes('ghost') || document.querySelector('meta[name="generator"][content*="Ghost"]')) {
                    result.cms = 'ghost';
                    result.platform = 'blog';
                } else if (html.includes('drupal') || document.querySelector('meta[name="generator"][content*="Drupal"]')) {
                    result.cms = 'drupal';
                    result.platform = 'news';
                }
                
                // í”Œë«í¼ ìœ í˜• ê°ì§€
                const titleAndMeta = (document.title + ' ' + headContent).toLowerCase();
                if (titleAndMeta.includes('news') || titleAndMeta.includes('journal') || titleAndMeta.includes('press')) {
                    result.platform = 'news';
                } else if (titleAndMeta.includes('blog') || titleAndMeta.includes('diary')) {
                    result.platform = 'blog';
                } else if (titleAndMeta.includes('forum') || titleAndMeta.includes('community')) {
                    result.platform = 'forum';
                }
                
                // êµ¬ì¡° ë¶„ì„
                const hasArticleTag = document.querySelectorAll('article').length > 0;
                const hasMainTag = document.querySelectorAll('main').length > 0;
                const hasPostClass = document.querySelectorAll('[class*="post"]').length > 0;
                const hasNewsClass = document.querySelectorAll('[class*="news"]').length > 0;
                
                if (hasArticleTag) {
                    result.structure = 'semantic';
                    result.features.push('html5-semantic');
                } else if (hasPostClass || hasNewsClass) {
                    result.structure = 'content-based';
                }
                
                // ë™ì  ì„ íƒì ìƒì„±
                const potentialArticleSelectors = [];
                const potentialContentSelectors = [];
                const potentialDateSelectors = [];
                
                // ê¸°ì‚¬ ë§í¬ê°€ ìˆì„ ë§Œí•œ ìš”ì†Œë“¤ ì°¾ê¸°
                const linkContainers = ['article', 'h1', 'h2', 'h3', '.post', '.news', '.story', '.entry', '.item'];
                linkContainers.forEach(selector => {
                    try {
                        const elements = document.querySelectorAll(selector + ' a');
                        if (elements.length > 0) {
                            potentialArticleSelectors.push(selector + ' a');
                        }
                    } catch (e) {}
                });
                
                // í´ë˜ìŠ¤ ê¸°ë°˜ ì„ íƒì ìƒì„±
                const classPatterns = ['title', 'headline', 'post', 'article', 'news', 'story', 'entry'];
                classPatterns.forEach(pattern => {
                    try {
                        const classSelector = '.' + pattern + ' a';
                        if (document.querySelectorAll(classSelector).length > 0) {
                            potentialArticleSelectors.push(classSelector);
                        }
                        const classOnlySelector = '.' + pattern + '-title a';
                        if (document.querySelectorAll(classOnlySelector).length > 0) {
                            potentialArticleSelectors.push(classOnlySelector);
                        }
                    } catch (e) {}
                });
                
                // ì½˜í…ì¸  ì„ íƒì ìƒì„±
                const contentContainers = ['article', 'main', '.content', '.post-content', '.entry-content', '.article-body'];
                contentContainers.forEach(selector => {
                    try {
                        if (document.querySelectorAll(selector).length > 0) {
                            potentialContentSelectors.push(selector);
                        }
                    } catch (e) {}
                });
                
                // ë‚ ì§œ ì„ íƒì ìƒì„±
                const dateContainers = ['time', '.date', '.published', '.timestamp', '.post-date', '.entry-date'];
                dateContainers.forEach(selector => {
                    try {
                        if (document.querySelectorAll(selector).length > 0) {
                            potentialDateSelectors.push(selector);
                        }
                    } catch (e) {}
                });
                
                result.selectors.articles = potentialArticleSelectors;
                result.selectors.content = potentialContentSelectors;
                result.selectors.dates = potentialDateSelectors;
                
                // ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘
                const ogSiteName = document.querySelector('meta[property="og:site_name"]');
                if (ogSiteName) result.meta_info.site_name = ogSiteName.content;
                
                const generator = document.querySelector('meta[name="generator"]');
                if (generator) result.meta_info.generator = generator.content;
                
                return result;
            }''')
            
            print(f"ì‚¬ì´íŠ¸ ê°ì§€ ê²°ê³¼: {detection_result['platform']}/{detection_result['cms']}")
            return detection_result
            
        except Exception as e:
            print(f"ì‚¬ì´íŠ¸ ê°ì§€ ì˜¤ë¥˜: {e}")
            return {'platform': 'unknown', 'cms': 'unknown', 'structure': 'unknown', 'features': [], 'selectors': {'articles': [], 'content': [], 'dates': []}}

# ìŠ¤í† ë¦¬ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
class Story:
    def __init__(self):
        self.headline = ""
        self.link = ""
        self.date_posted = ""
        self.fullContent = ""
        self.imageUrls = []
        self.videoUrls = []
        self.popularity = ""
        self.summary = ""  # ìš”ì•½ ì¶”ê°€
        self.tags = []     # íƒœê·¸ ì¶”ê°€
        self.source = ""   # ì†ŒìŠ¤ ì‚¬ì´íŠ¸ ì¶”ê°€
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "headline": self.headline,
            "link": self.link,
            "date_posted": self.date_posted,
            "fullContent": self.fullContent,
            "imageUrls": self.imageUrls,
            "videoUrls": self.videoUrls,
            "popularity": self.popularity,
            "summary": self.summary,
            "tags": self.tags,
            "source": self.source
        }

# í¬ë¡¤ë§ ê²°ê³¼ í´ë˜ìŠ¤
class CrawlResult:
    def __init__(self, source: str):
        self.source = source
        self.stories = []
        self.error = None
        self.site_info = {}  # ì‚¬ì´íŠ¸ ì •ë³´ ì¶”ê°€
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "source": self.source,
            "stories": [story.to_dict() for story in self.stories],
            "error": self.error,
            "site_info": self.site_info
        }

# í–¥ìƒëœ ë‚ ì§œ íŒŒì‹± í•¨ìˆ˜
def parse_date(date_str: str) -> str:
    if not date_str:
        return ""

    original_date_str = date_str.strip()
    now = datetime.now()

    # 1. "ì˜¤ëŠ˜", "Today", "just now" ì²˜ë¦¬
    today_patterns = ['today', 'ì˜¤ëŠ˜', 'just now', 'now', 'a moment ago', 'ë°©ê¸ˆ', 'ì§€ê¸ˆ']
    if any(pattern in original_date_str.lower() for pattern in today_patterns):
        return now.strftime('%Y-%m-%d')

    # 2. "ì–´ì œ", "Yesterday" ì²˜ë¦¬
    yesterday_patterns = ['yesterday', 'ì–´ì œ']
    if any(pattern in original_date_str.lower() for pattern in yesterday_patterns):
        yesterday = now - timedelta(days=1)
        return yesterday.strftime('%Y-%m-%d')

    # 3. ìƒëŒ€ì  ì‹œê°„ í‘œí˜„ ì²˜ë¦¬ (ë” ë§ì€ íŒ¨í„´)
    relative_patterns = [
        (r'(\d+)\s*(ì´ˆ|second)s?\s*(ì „|ago)', 'seconds'),
        (r'(\d+)\s*(ë¶„|minute|min)s?\s*(ì „|ago)', 'minutes'),
        (r'(\d+)\s*(ì‹œê°„|hour|hr)s?\s*(ì „|ago)', 'hours'),
        (r'(\d+)\s*(ì¼|day)s?\s*(ì „|ago)', 'days'),
        (r'(\d+)\s*(ì£¼|week)s?\s*(ì „|ago)', 'weeks'),
        (r'(\d+)\s*(ë‹¬|month)s?\s*(ì „|ago)', 'months'),
        (r'(\d+)\s*(ë…„|year)s?\s*(ì „|ago)', 'years'),
    ]
    
    for pattern, unit in relative_patterns:
        match = re.search(pattern, original_date_str, re.IGNORECASE)
        if match:
            value = int(match.group(1))
            
            if unit == 'seconds':
                target_date = now - timedelta(seconds=value)
            elif unit == 'minutes':
                target_date = now - timedelta(minutes=value)
            elif unit == 'hours':
                target_date = now - timedelta(hours=value)
            elif unit == 'days':
                target_date = now - timedelta(days=value)
            elif unit == 'weeks':
                target_date = now - timedelta(weeks=value)
            elif unit == 'months':
                # ì›” ê³„ì‚° (ê·¼ì‚¬ì¹˜)
                target_date = now - timedelta(days=value * 30)
            elif unit == 'years':
                try:
                    target_date = now.replace(year=now.year - value)
                except ValueError:
                    target_date = now.replace(year=now.year - value, day=28)
            
            return target_date.strftime('%Y-%m-%d')
    
    # 4. ISO 8601 ë° ìœ ì‚¬ í˜•ì‹ ì²˜ë¦¬
    try:
        # Tì™€ Zê°€ í¬í•¨ëœ ISO í˜•ì‹
        iso_cleaned = original_date_str.replace(' ', 'T')
        dt_obj = date_parser.isoparse(iso_cleaned)
        return dt_obj.strftime('%Y-%m-%d')
    except (ValueError, TypeError):
        pass
    
    # 5. ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì‹œë„ (ë” ë§ì€ í˜•ì‹ ì¶”ê°€)
    formats = [
        # ê¸°ë³¸ í˜•ì‹ë“¤
        '%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%Y.%m.%d %H:%M:%S',
        '%Y-%m-%d', '%Y/%m/%d', '%d-%m-%Y', '%d/%m/%Y', '%m/%d/%Y',
        '%Y.%m.%d', '%d.%m.%Y', '%m.%d.%Y',
        
        # ì˜ì–´ ì›” ì´ë¦„
        '%b %d, %Y', '%B %d, %Y', '%d %b %Y', '%d %B %Y',
        '%b %d %Y', '%B %d %Y', '%Y %b %d', '%Y %B %d',
        
        # í•œêµ­ì–´ í˜•ì‹
        '%Yë…„ %mì›” %dì¼', '%Y. %m. %d.', '%Y. %m. %d', '%y.%m.%d', '%Yë…„%mì›”%dì¼',
        
        # íŠ¹ìˆ˜ í˜•ì‹ë“¤
        '%Y%m%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ',
        '%Y. %m. %d. %H:%M', '%Y/%m/%d %H:%M', '%d/%m/%Y %H:%M',
        
        # ì¶”ê°€ ì˜ì–´ í˜•ì‹
        '%a, %d %b %Y', '%A, %B %d, %Y', '%d-%b-%Y',
        
        # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ í˜•ì‹
        '%Y%m%d%H%M%S', '%Y%m%d%H%M',
    ]
    
    for fmt in formats:
        try:
            date_obj = datetime.strptime(original_date_str, fmt)
            return date_obj.strftime('%Y-%m-%d')
        except ValueError:
            continue
    
    # 6. dateutil.parserë¡œ ìµœì¢… ì‹œë„ (ë” ê´€ëŒ€í•œ ì„¤ì •)
    try:
        parsed_date = date_parser.parse(original_date_str, fuzzy=True, dayfirst=False, yearfirst=True)
        return parsed_date.strftime('%Y-%m-%d')
    except (ValueError, TypeError, OverflowError) as e:
        print(f"ë‚ ì§œ íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: '{original_date_str}' ({e})")
        return original_date_str

# í–¥ìƒëœ ë‚ ì§œ ê´€ë ¨ì„± í™•ì¸
def is_relevant_date(date_str: str, target_date: Optional[str] = None, timeframe_hours: int = 48) -> bool:
    if not date_str:
        print("ğŸ“… ë‚ ì§œ ì •ë³´ ì—†ìŒ - ê´€ëŒ€í•˜ê²Œ í¬í•¨í•¨")
        return True  # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ í¬í•¨ (ë” ê´€ëŒ€í•œ ì •ì±…)

    now_utc = datetime.now(timezone.utc)
    cutoff_utc = now_utc - timedelta(hours=timeframe_hours)
    
    print(f"ğŸ“… ë‚ ì§œ í™•ì¸: '{date_str}', ê¸°ì¤€: {timeframe_hours}ì‹œê°„ ì´ë‚´")
    
    try:
        # ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ì²˜ë¦¬
        parsed_date_str = parse_date(date_str)
        if not parsed_date_str or parsed_date_str == date_str:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë” ê´€ëŒ€í•˜ê²Œ ì²˜ë¦¬
            print(f"ğŸ“… ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•˜ì§€ë§Œ í¬í•¨: {date_str}")
            return True
        
        # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ íŒŒì‹±ëœ ê²½ìš°
        if re.match(r'\d{4}-\d{2}-\d{2}', parsed_date_str):
            date_obj = datetime.strptime(parsed_date_str, '%Y-%m-%d').replace(tzinfo=timezone.utc)
            
            # target_dateê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë‚ ì§œ ì´í›„ì¸ì§€ í™•ì¸
            if target_date:
                target_date_obj = datetime.strptime(target_date, '%Y-%m-%d').replace(tzinfo=timezone.utc)
                if date_obj < target_date_obj:
                    print(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ({target_date}) ì´ì „ì˜ ì½˜í…ì¸  - ì œì™¸")
                    return False
            
            # timeframe_hours ì´ë‚´ì¸ì§€ í™•ì¸ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
            # ë‚ ì§œë§Œ ìˆëŠ” ê²½ìš° í•˜ë£¨ ì „ì²´ë¥¼ ì»¤ë²„í•˜ë„ë¡ 24ì‹œê°„ ì—¬ìœ ë¥¼ ì¤Œ
            extended_cutoff = cutoff_utc - timedelta(hours=24)
            is_recent = date_obj >= extended_cutoff
            
            if not is_recent:
                print(f"ğŸ“… ì‹œê°„ ë²”ìœ„ ì´ˆê³¼: {date_obj.isoformat()} < {extended_cutoff.isoformat()}")
                # í•˜ì§€ë§Œ ì¼ì£¼ì¼ ì´ë‚´ë¼ë©´ í¬í•¨ (ë°±ì—… ì •ì±…)
                week_cutoff = now_utc - timedelta(days=7)
                if date_obj >= week_cutoff:
                    print(f"ğŸ“… ë°±ì—… ì •ì±…: ì¼ì£¼ì¼ ì´ë‚´ì´ë¯€ë¡œ í¬í•¨")
                    return True
                return False
            
            print(f"ğŸ“… ë‚ ì§œ í•„í„° í†µê³¼: {date_obj.isoformat()}")
            return True
        
        return True  # ê¸°íƒ€ ê²½ìš°ëŠ” í¬í•¨
        
    except Exception as e:
        print(f"ğŸ“… ë‚ ì§œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)} - í¬í•¨í•¨")
        return True

# Playwrightë¥¼ ì‚¬ìš©í•œ ë™ì  í¬ë¡¤ë§ í´ë˜ìŠ¤ (ëŒ€í­ ê°œì„ )
class DynamicCrawler:
    def __init__(self, 
                 llm_provider: str = 'openai', 
                 headless: bool = True, 
                 target_date: Optional[str] = None):
        self.llm_provider = llm_provider
        self.headless = headless
        self.target_date = target_date
        self.browser = None
        self.context = None
        self.base_host = None
        # ì‚¬ì´íŠ¸ ê°ì§€ ê²°ê³¼ ìºì‹±
        self.site_detection_cache = {}
        
        print("ğŸš€ ê°„ì†Œí™”ëœ ì‚¬ì´íŠ¸ë³„ íŠ¹í™” í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”")
    
    def _load_json_configurations(self):
        """JSON ì„¤ì • íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        print("ğŸ“‹ ê°„ì†Œí™”ëœ ë²„ì „: JSON ì„¤ì • ë¡œë” ë¹„í™œì„±í™”ë¨")
        return
    
    def _add_target_site_configs(self):
        """í˜„ì¬ íƒ€ê²Ÿ ì‚¬ì´íŠ¸ë“¤ì— ëŒ€í•œ íŠ¹í™”ëœ ì„¤ì •ì„ ì¶”ê°€"""
        print("ğŸ“‹ ê°„ì†Œí™”ëœ ë²„ì „: ì‚¬ì´íŠ¸ë³„ íŠ¹í™” í¬ë¡¤ë§ ì‚¬ìš©")
        return
    
    def _print_loaded_configurations(self):
        """ë¡œë“œëœ ì„¤ì •ë“¤ì˜ ìš”ì•½ì„ ì¶œë ¥"""
        print("ğŸ”§ ê°„ì†Œí™”ëœ ë²„ì „: ì‚¬ì´íŠ¸ë³„ íŠ¹í™” í¬ë¡¤ëŸ¬")
        return
    
    def add_custom_site_config(self, domain: str, selectors: Dict[str, List[str]]):
        """ëŸ°íƒ€ì„ì— ì»¤ìŠ¤í…€ ì‚¬ì´íŠ¸ ì„¤ì •ì„ ì¶”ê°€í•˜ëŠ” ë©”ì„œë“œ"""
        print(f"âš ï¸ ê°„ì†Œí™”ëœ ë²„ì „: ì»¤ìŠ¤í…€ ì„¤ì • ë¹„í™œì„±í™”ë¨ ({domain})")
        return
    
    def _extract_domain_name(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ëª… ì¶”ì¶œ"""
        try:
            parsed = urllib.parse.urlparse(url)
            domain = parsed.netloc.lower()
            # www. ì œê±°
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except Exception:
            return url
    
    def validate_site_config(self, domain: str) -> bool:
        """ì‚¬ì´íŠ¸ ì„¤ì •ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” ë©”ì„œë“œ"""
        print(f"âœ… ê°„ì†Œí™”ëœ ë²„ì „: {domain} ì„¤ì • ê²€ì¦ í†µê³¼")
        return True
    
    async def initialize(self):
        """ë¸Œë¼ìš°ì € ì´ˆê¸°í™” (ë” ì•ˆì •ì ì¸ ì„¤ì •)"""
        try:
            self.playwright = await async_playwright().start()
            
            # ë” ì•ˆì •ì ì¸ ë¸Œë¼ìš°ì € ì„¤ì •
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-extensions',
                    '--no-first-run',
                    '--no-default-browser-check',
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-renderer-backgrounding'
                ]
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ê°œì„ 
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                java_script_enabled=True,
                accept_downloads=False,
                ignore_https_errors=True,
                bypass_csp=True,  # CSP ìš°íšŒ
                extra_http_headers={
                    'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                }
            )
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            self.context.set_default_navigation_timeout(90000)  # 90ì´ˆ
            self.context.set_default_timeout(45000)  # 45ì´ˆ
            
            print("ë¸Œë¼ìš°ì € ë° ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            raise
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
    
    async def handle_popups_and_overlays(self, page: Page) -> None:
        """í–¥ìƒëœ íŒì—… ë° ì˜¤ë²„ë ˆì´ ì²˜ë¦¬"""
        try:
            # ì¼ë°˜ì ì¸ íŒì—… ì„ íƒìë“¤
            popup_selectors = [
                # ì¿ í‚¤ ê´€ë ¨
                "button:has-text('Accept')", "button:has-text('Agree')", 
                "button:has-text('Accept Cookies')", "button:has-text('Allow Cookies')",
                "button:has-text('Accept All')", "button:has-text('Got it')",
                "button:has-text('OK')", "button:has-text('í™•ì¸')",
                "button[id*='cookie'][id*='accept']", "button[class*='cookie'][class*='accept']",
                
                # GDPR ê´€ë ¨
                "button:has-text('I Understand')", "button:has-text('Understood')",
                "button:has-text('Continue')", "button:has-text('Proceed')",
                
                # ë‰´ìŠ¤ë ˆí„°/êµ¬ë… ê´€ë ¨
                "button:has-text('Maybe Later')", "button:has-text('No Thanks')",
                "button:has-text('Skip')", "button:has-text('Close')",
                "[aria-label*='close']", "[aria-label*='dismiss']",
                
                # ì¼ë°˜ì ì¸ ë‹«ê¸° ë²„íŠ¼
                ".close", ".dismiss", ".popup-close", ".modal-close",
                "[data-dismiss]", "[data-close]",
                
                # íŠ¹ì • ì‚¬ì´íŠ¸ íŒ¨í„´
                "div[role='dialog'] button", "div[aria-modal='true'] button"
            ]
            
            # ESC í‚¤ ì‹œë„
            await page.keyboard.press('Escape')
            await page.wait_for_timeout(1000)
            
            # ê° ì„ íƒì ì‹œë„
            for selector in popup_selectors:
                try:
                    elements = page.locator(selector)
                    count = await elements.count()
                    
                    for i in range(min(count, 3)):  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì‹œë„
                        element = elements.nth(i)
                        if await element.is_visible() and await element.is_enabled():
                            await element.click(timeout=3000)
                            print(f"íŒì—… ë²„íŠ¼ í´ë¦­: {selector}")
                            await page.wait_for_timeout(1000)
                            break
                except Exception:
                    continue
            
            # ì˜¤ë²„ë ˆì´ ì œê±° (JavaScript)
            await page.evaluate(r'''() => {
                // ê³ ì • ìœ„ì¹˜ ì˜¤ë²„ë ˆì´ ì œê±°
                const overlays = document.querySelectorAll('[style*="position: fixed"], [style*="position:fixed"]');
                overlays.forEach(el => {
                    const zIndex = window.getComputedStyle(el).zIndex;
                    if (parseInt(zIndex) > 1000) {
                        el.style.display = 'none';
                    }
                });
                
                // ì¼ë°˜ì ì¸ ì˜¤ë²„ë ˆì´ í´ë˜ìŠ¤ ì œê±°
                const overlayClasses = ['.overlay', '.modal-backdrop', '.popup-overlay', '.cookie-banner'];
                overlayClasses.forEach(className => {
                    const elements = document.querySelectorAll(className);
                    elements.forEach(el => el.style.display = 'none');
                });
            }''')
            
        except Exception as e:
            print(f"íŒì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def analyze_page_structure(self, page: Page, url: str) -> Dict[str, Any]:
        """í–¥ìƒëœ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„"""
        try:
            # ì‚¬ì´íŠ¸ ê°ì§€ (ìºì‹± ì‚¬ìš©)
            if url not in self.site_detection_cache:
                self.site_detection_cache[url] = await SiteDetector.detect_site_type(page)
            
            site_detection = self.site_detection_cache[url]
            
            # ì‚¬ì´íŠ¸ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            self.current_site_config = site_config_manager.get_config(url)
            print(f"ì‚¬ì´íŠ¸ ì„¤ì • ì ìš©: {self.current_site_config.domain} (ê°ì§€ëœ ìœ í˜•: {site_detection['platform']}/{site_detection['cms']})")
            
            # ë™ì ìœ¼ë¡œ ê°ì§€ëœ ì„ íƒìì™€ ê¸°ì¡´ ì„¤ì • ê²°í•©
            universal_selectors = site_config_manager.get_universal_selectors()
            detected_selectors = site_detection['selectors']
            
            # ì„ íƒì ìš°ì„ ìˆœìœ„: ì‚¬ì´íŠ¸ë³„ > ê°ì§€ëœ ê²ƒ > ë²”ìš©
            article_selectors = (
                self.current_site_config.article_selectors +
                detected_selectors['articles'] +
                universal_selectors['article_selectors']
            )
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
            article_selectors = list(dict.fromkeys(article_selectors))
            
            # í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ (ë” ìŠ¤ë§ˆíŠ¸í•œ ë°©ì‹)
            links = await page.evaluate(r'''(selectors, siteType) => {
                console.log('ğŸ” ì‚¬ìš©í•  ì„ íƒìë“¤:', selectors);
                console.log('ğŸ›ï¸ ì‚¬ì´íŠ¸ ìœ í˜•:', siteType);
                
                let allLinks = [];
                const processedHrefs = new Set();
                let totalElements = 0;
                
                // ê° ì„ íƒìì— ëŒ€í•´ ë§í¬ ìˆ˜ì§‘
                for (const selector of selectors) {
                    try {
                        const elements = document.querySelectorAll(selector);
                        totalElements += elements.length;
                        console.log(`ğŸ” ì„ íƒì "${selector}": ${elements.length}ê°œ ìš”ì†Œ ë°œê²¬`);
                        
                        elements.forEach((link, index) => {
                            const href = link.getAttribute('href');
                            if (!href || processedHrefs.has(href)) return;
                            
                            // ë§í¬ í’ˆì§ˆ ê²€ì‚¬ (ë” ê´€ëŒ€í•˜ê²Œ)
                            if (href.startsWith('javascript:') || 
                                href.startsWith('mailto:') || 
                                href.startsWith('tel:') ||
                                href === '#' ||
                                href.length < 3) {
                                return;
                            }
                            
                            processedHrefs.add(href);
                            
                            // ë” ì •êµí•œ ë‚ ì§œ ì¶”ì¶œ
                            let dateStr = '';
                            
                            // 1. ë§í¬ê°€ ì†í•œ ì»¨í…Œì´ë„ˆì—ì„œ ë‚ ì§œ ì°¾ê¸°
                            const containers = [
                                'article', '.post', '.entry', '.news-item', '.story', '.item',
                                '[role="listitem"]', '[role="article"]', '.content-item', 
                                '.blog-post', '.post-item', '.news-post', 'li', '.card'  // ì¶”ê°€ ì»¨í…Œì´ë„ˆ
                            ];
                            
                            let container = null;
                            for (const containerSelector of containers) {
                                container = link.closest(containerSelector);
                                if (container) break;
                            }
                            
                            if (container) {
                                // ë‚ ì§œ ì„ íƒìë“¤ (í™•ì¥ë¨)
                                const dateSelectors = [
                                    'time', '[datetime]', '.date', '.published', '.timestamp',
                                    '.post-date', '.article-date', '.entry-date', '.published-date',
                                    '.byline time', '.meta time', '[data-date]', '.age', '.subtext',
                                    'span[title*="20"]', 'span[aria-label*="20"]', 'abbr.published',
                                    '.timestamp-text', 'span[class*="date"]'  // ì¶”ê°€ ì„ íƒì
                                ];
                                
                                for (const dateSelector of dateSelectors) {
                                    const dateEl = container.querySelector(dateSelector);
                                    if (dateEl) {
                                        let text = dateEl.getAttribute('datetime') || 
                                                  dateEl.getAttribute('title') || 
                                                  dateEl.getAttribute('data-date') ||
                                                  dateEl.textContent;
                                        
                                        if (text && text.trim()) {
                                            text = text.trim();
                                            // ë‚ ì§œ íŒ¨í„´ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
                                            if (/\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b/i.test(text) ||
                                                /\\b(\\d{1,2})\\s*(hour|day|week|month|year)s?\\s*(ago|ì „)\\b/i.test(text) ||
                                                /\\b(today|yesterday|ì˜¤ëŠ˜|ì–´ì œ|ë°©ê¸ˆ|now|ì§€ê¸ˆ)\\b/i.test(text) ||
                                                /\\d{4}-\\d{2}-\\d{2}/.test(text) ||
                                                /\\d{1,2}\\s*(ì‹œê°„|ì¼|ì£¼|ê°œì›”)\\s*(ì „|ago)/.test(text)) {
                                                dateStr = text;
                                                break;
                                            }
                                        }
                                    }
                                }
                                
                                // ë°±ì—…: í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (ë” ë§ì€ íŒ¨í„´)
                                if (!dateStr) {
                                    const textContent = container.textContent || '';
                                                                    const datePatterns = [
                                    /\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},?\\s+\\d{4}\\b/g,
                                    /\\b\\d{1,2}\\s*(hour|day|week|month)s?\\s*ago\\b/g,
                                    /\\b(today|yesterday|ë°©ê¸ˆ|ì§€ê¸ˆ|ì˜¤ëŠ˜|ì–´ì œ)\\b/g,
                                    /\\d{4}-\\d{2}-\\d{2}/g,
                                    /\\d{1,2}\\s*(ì‹œê°„|ì¼|ì£¼|ê°œì›”)\\s*ì „/g
                                ];
                                    
                                    for (const pattern of datePatterns) {
                                        const match = textContent.match(pattern);
                                        if (match) {
                                            dateStr = match[0];
                                            break;
                                        }
                                    }
                                }
                            }
                            
                            // ë§í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë” ìŠ¤ë§ˆíŠ¸í•˜ê²Œ)
                            let linkText = (link.innerText || link.textContent || '').trim();
                            if (!linkText) {
                                // ì´ë¯¸ì§€ alt í…ìŠ¤íŠ¸ë‚˜ title ì†ì„±ë„ í™•ì¸
                                const img = link.querySelector('img');
                                if (img) {
                                    linkText = img.getAttribute('alt') || img.getAttribute('title') || '';
                                }
                                if (!linkText) {
                                    linkText = link.getAttribute('title') || link.getAttribute('aria-label') || '';
                                }
                            }
                            
                            // ë§í¬ ì •ë³´ ìƒì„±
                            const linkData = {
                                href: href,
                                text: linkText,
                                classes: link.className || '',
                                id: link.id || '',
                                date: dateStr,
                                selector_used: selector,
                                container_tag: container ? container.tagName.toLowerCase() : 'none'
                            };
                            
                            // í…ìŠ¤íŠ¸ ê¸¸ì´ í•„í„°ë§ (ë” ê´€ëŒ€í•˜ê²Œ)
                            if (linkData.text.length >= 2 && linkData.text.length <= 300) {  // 5ìì—ì„œ 2ìë¡œ, 200ìì—ì„œ 300ìë¡œ
                                allLinks.push(linkData);
                            }
                        });
                    } catch (e) {
                        console.warn('âš ï¸ ì„ íƒì ì²˜ë¦¬ ì˜¤ë¥˜:', selector, e);
                    }
                }
                
                console.log(`ğŸ“Š í†µê³„: ì´ ${totalElements}ê°œ ìš”ì†Œ ê²€ì‚¬, ${allLinks.length}ê°œ ìœ íš¨ ë§í¬ ì¶”ì¶œ`);
                
                // ë°±ì—…: ê¸°ë³¸ ë§í¬ ì¶”ì¶œ (ì•„ë¬´ê²ƒë„ ì°¾ì§€ ëª»í•œ ê²½ìš°)
                if (allLinks.length === 0) {
                    console.log('ğŸ”„ ë°±ì—… ë§í¬ ì¶”ì¶œ ì‹œë„...');
                    const fallbackSelectors = ['a[href]', 'a'];
                    
                    for (const selector of fallbackSelectors) {
                        const links = document.querySelectorAll(selector);
                        console.log(`ğŸ”„ ë°±ì—… ì„ íƒì "${selector}": ${links.length}ê°œ ë§í¬`);
                        
                        Array.from(links).slice(0, 50).forEach(link => {  // ìµœëŒ€ 50ê°œë§Œ
                            const href = link.getAttribute('href');
                            const text = (link.textContent || '').trim();
                            
                            if (href && href.length > 3 && text.length > 2 && 
                                !href.startsWith('javascript:') && 
                                !href.startsWith('mailto:') && 
                                !processedHrefs.has(href)) {
                                
                                processedHrefs.add(href);
                                allLinks.push({
                                    href: href,
                                    text: text,
                                    classes: link.className || '',
                                    id: link.id || '',
                                    date: '',
                                    selector_used: 'fallback:' + selector,
                                    container_tag: 'fallback'
                                });
                            }
                        });
                        
                        if (allLinks.length > 0) break;
                    }
                }
                
                // í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
                allLinks.forEach(link => {
                    let score = 0;
                    
                    // ë‚ ì§œê°€ ìˆìœ¼ë©´ ê°€ì 
                    if (link.date) score += 5;
                    
                    // ì œëª© í’ˆì§ˆ ì ìˆ˜
                    const titleWords = link.text.split(/\\s+/).length;
                    if (titleWords >= 2 && titleWords <= 20) score += 3;  // 3ì—ì„œ 2ë¡œ ì™„í™”
                    
                    // URL í’ˆì§ˆ ì ìˆ˜
                    if (link.href.includes('/article/') || 
                        link.href.includes('/post/') || 
                        link.href.includes('/story/') ||
                        link.href.includes('/blog/') ||
                        link.href.includes('/news/') ||
                        link.href.includes('/discover/') ||
                        link.href.includes('/item?id=')) {
                        score += 2;
                    }
                    
                    // ì»¨í…Œì´ë„ˆ í’ˆì§ˆ ì ìˆ˜
                    if (link.container_tag === 'article' || 
                        link.container_tag === 'li' ||
                        link.container_tag === 'div') {
                        score += 1;
                    }
                    
                    link.quality_score = score;
                });
                
                // í’ˆì§ˆ ì ìˆ˜ì™€ ë‚ ì§œ ê¸°ì¤€ ì •ë ¬
                return allLinks
                    .sort((a, b) => {
                        // ë¨¼ì € í’ˆì§ˆ ì ìˆ˜ë¡œ ì •ë ¬
                        if (b.quality_score !== a.quality_score) {
                            return b.quality_score - a.quality_score;
                        }
                        // ê°™ì€ í’ˆì§ˆì´ë©´ ë‚ ì§œë¡œ ì •ë ¬
                        if (a.date && b.date) {
                            try {
                                return new Date(b.date) - new Date(a.date);
                            } catch (e) {
                                return 0;
                            }
                        }
                        return a.date ? -1 : (b.date ? 1 : 0);
                    });
            }''', article_selectors, site_detection['platform'])
            
            print(f"ğŸ” ë¶„ì„ ì™„ë£Œ: {len(links)}ê°œ ë§í¬ ì¶”ì¶œ (ì‚¬ì´íŠ¸ ìœ í˜•: {site_detection['platform']})")
            
            # ë§í¬ê°€ ì—†ìœ¼ë©´ ë” ìƒì„¸í•œ ì§„ë‹¨ ì •ë³´ ì¶œë ¥
            if len(links) == 0:
                print("âš ï¸ ê²½ê³ : ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨!")
                print("ğŸ” í˜ì´ì§€ ì§„ë‹¨ ì •ë³´:")
                
                # í˜ì´ì§€ ê¸°ë³¸ ì •ë³´ í™•ì¸
                page_info = await page.evaluate(r'''() => {
                    return {
                        title: document.title,
                        url: window.location.href,
                        totalLinks: document.querySelectorAll('a').length,
                        articles: document.querySelectorAll('article').length,
                        h1Count: document.querySelectorAll('h1').length,
                        h2Count: document.querySelectorAll('h2').length,
                        h3Count: document.querySelectorAll('h3').length,
                        hasMain: document.querySelectorAll('main').length > 0,
                        bodyClass: document.body ? document.body.className : 'none'
                    };
                }''')
                
                print(f"   ğŸ“„ í˜ì´ì§€ ì œëª©: {page_info['title']}")
                print(f"   ğŸ”— ì´ ë§í¬ ìˆ˜: {page_info['totalLinks']}")
                print(f"   ğŸ“° Article íƒœê·¸: {page_info['articles']}ê°œ")
                print(f"   ğŸ“ ì œëª© íƒœê·¸: H1({page_info['h1Count']}), H2({page_info['h2Count']}), H3({page_info['h3Count']})")
                print(f"   ğŸ—ï¸ Main íƒœê·¸: {'ìˆìŒ' if page_info['hasMain'] else 'ì—†ìŒ'}")
                print(f"   ğŸ¨ Body í´ë˜ìŠ¤: {page_info['bodyClass'][:100]}...")
            
            else:
                # ì„±ê³µ ì‹œ ë§í¬ í’ˆì§ˆ ë¶„í¬ ì¶œë ¥
                quality_distribution = {}
                for link in links[:10]:  # ìƒìœ„ 10ê°œë§Œ í™•ì¸
                    score = link.get('quality_score', 0)
                    quality_distribution[score] = quality_distribution.get(score, 0) + 1
                
                print(f"ğŸ¯ ìƒìœ„ ë§í¬ í’ˆì§ˆ ë¶„í¬: {quality_distribution}")

            # URL ì •ê·œí™”ë¥¼ ìœ„í•´ í˜„ì¬ í˜ì´ì§€ì˜ í˜¸ìŠ¤íŠ¸ ì €ì¥
            parsed_url = urllib.parse.urlparse(url)
            self.base_host = f"{parsed_url.scheme}://{parsed_url.netloc}"

            # ì½˜í…ì¸  ì„ íƒì ê²°í•©
            content_selectors = (
                self.current_site_config.content_selectors +
                detected_selectors['content'] +
                universal_selectors['content_selectors']
            )
            content_selector_str = ', '.join(list(dict.fromkeys(content_selectors)))

            return {
                'url': url,
                'links': links,
                'structure': {
                    'contentSelector': content_selector_str, 
                    'hasContent': True,
                    'siteType': site_detection
                }
            }
            
        except Exception as e:
            print(f"í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'url': url, 
                'links': [], 
                'structure': {
                    'contentSelector': None, 
                    'hasContent': False,
                    'siteType': {'platform': 'unknown', 'cms': 'unknown'}
                }
            }
    
    async def filter_relevant_links(self, links: List[Dict[str, Any]], content_focus: Optional[str] = None) -> List[Dict[str, Any]]:
        """í–¥ìƒëœ ë§í¬ í•„í„°ë§ (í’ˆì§ˆ ê¸°ë°˜, ë” ê´€ëŒ€í•œ ì •ì±…)"""
        if not links:
            return []

        print(f"ğŸ”— í•„í„°ë§ ì „ ë§í¬ ìˆ˜: {len(links)}")

        # í˜„ì¬ ì‚¬ì´íŠ¸ ì„¤ì • ì‚¬ìš©
        site_config = self.current_site_config or site_config_manager.get_config(self.base_host or "")

        filtered_links = []
        included_by_pattern = 0
        excluded_by_pattern = 0

        for i, link_data in enumerate(links):
            href = link_data.get('href', '')
            if not href:
                continue
            
            # URL ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            try:
                if self.base_host and not href.startswith(('http://', 'https://')):
                    normalized_href = urllib.parse.urljoin(self.base_host, href)
                    href = normalized_href
                    link_data['href'] = href
            except Exception as e:
                print(f"âŒ URL ì •ê·œí™” ì¤‘ ì˜¤ë¥˜: {e}")
                continue
            
            link_text = link_data.get('text', '')
            
            # ì‚¬ì´íŠ¸ë³„ í¬í•¨ íŒ¨í„´ í™•ì¸ (ìš°ì„ ìˆœìœ„)
            included = False
            for pattern in site_config.included_url_patterns:
                if pattern in href.lower():
                    filtered_links.append(link_data)
                    included = True
                    included_by_pattern += 1
                    print(f"âœ… í¬í•¨ íŒ¨í„´ ë§¤ì¹­ [{i+1}]: {pattern} in {href[:60]}...")
                    break
            
            if included:
                continue
                
            # ì œì™¸ íŒ¨í„´ í™•ì¸ (ê¸°ë³¸ ì œì™¸ ëª©ë¡ì„ ë” ì œí•œì ìœ¼ë¡œ)
            excluded = False
            basic_excluded_patterns = [
                '/login', '/register', '/signup', '/subscribe', '/account',
                '/profile', '/settings', '/privacy', '/terms', '/contact',
                '/about', '/search', '/feed/', '/rss/', '/xml', '/sitemap',
                '/wp-admin', '/admin', '/ads/', '/advertisement'
            ]
            
            # ì‚¬ì´íŠ¸ë³„ ì œì™¸ íŒ¨í„´ + ê¸°ë³¸ ì œì™¸ íŒ¨í„´
            all_excluded_patterns = site_config.excluded_url_patterns + basic_excluded_patterns
            
            for pattern in all_excluded_patterns:
                if pattern in href.lower():
                    excluded = True
                    excluded_by_pattern += 1
                    print(f"âŒ ì œì™¸ íŒ¨í„´ ë§¤ì¹­ [{i+1}]: {pattern} in {href[:60]}...")
                    break
            
            # ì¶”ê°€ íœ´ë¦¬ìŠ¤í‹± í•„í„°ë§ (ë” ê´€ëŒ€í•˜ê²Œ)
            if not excluded:
                # ë„ˆë¬´ ì§§ì€ ë§í¬ í…ìŠ¤íŠ¸ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
                if len(link_text.strip()) < 3:
                    print(f"âš ï¸ ë§í¬ í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ [{i+1}]: '{link_text}' - ì œì™¸")
                    continue
                
                # ëª…ë°±íˆ ì½˜í…ì¸ ê°€ ì•„ë‹Œ ë§í¬ë“¤ (ë” ì œí•œì ìœ¼ë¡œ)
                non_content_keywords = ['javascript:', 'mailto:', 'tel:', '#top', '#bottom', 'void(0)']
                if any(keyword in href.lower() for keyword in non_content_keywords):
                    print(f"âš ï¸ ë¹„ì½˜í…ì¸  ë§í¬ [{i+1}]: {href[:60]}... - ì œì™¸")
                    continue
                
                filtered_links.append(link_data)
                print(f"âœ… ì¼ë°˜ í•„í„° í†µê³¼ [{i+1}]: {link_text[:40]}...")

        print(f"ğŸ”— ê¸°ë³¸ í•„í„°ë§ í›„ ë§í¬ ìˆ˜: {len(filtered_links)} (í¬í•¨íŒ¨í„´: {included_by_pattern}, ì œì™¸íŒ¨í„´: {excluded_by_pattern})")
        
        # ì½˜í…ì¸  ê´€ë ¨ì„± í•„í„°ë§ (ë” ê´€ëŒ€í•˜ê²Œ)
        if content_focus and len(filtered_links) > 10:  # ë§í¬ê°€ ë§ì„ ë•Œë§Œ ì ìš©
            print(f"ğŸ¯ ì½˜í…ì¸  ê´€ë ¨ì„± í•„í„°ë§ ì ìš©: '{content_focus}'")
            relevant_keywords = content_focus.lower().split()
            scored_links = []
            
            for link in filtered_links:
                score = link.get('quality_score', 0)
                text = link.get('text', '').lower()
                href = link.get('href', '').lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ì¶”ê°€
                for keyword in relevant_keywords:
                    if keyword in text:
                        score += 5  # ì œëª©ì— í‚¤ì›Œë“œ
                    if keyword in href:
                        score += 2  # URLì— í‚¤ì›Œë“œ
                
                # ê¸°ìˆ /ë‰´ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€ ì ìˆ˜
                tech_keywords = ['ai', 'artificial intelligence', 'tech', 'technology', 
                               'innovation', 'startup', 'digital', 'software', 'hardware', 
                               'machine learning', 'deep learning', 'neural', 'llm', 'gpt']
                for keyword in tech_keywords:
                    if keyword in text or keyword in href:
                        score += 1
                
                scored_links.append((link, score))
            
            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            scored_links.sort(key=lambda x: x[1], reverse=True)
            final_links = [link for link, _ in scored_links[:25]]  # ìƒìœ„ 25ê°œë¡œ í™•ëŒ€
            
        else:
            # í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            filtered_links.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
            final_links = filtered_links[:20]  # ìƒìœ„ 20ê°œë¡œ í™•ëŒ€
        
        print(f"ğŸ¯ ìµœì¢… ì„ íƒëœ ë§í¬ ìˆ˜: {len(final_links)}")
        
        # ì„ íƒëœ ë§í¬ë“¤ì˜ ê°„ë‹¨í•œ ì •ë³´ ì¶œë ¥
        for i, link in enumerate(final_links[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            print(f"   {i}. [{link.get('quality_score', 0)}ì ] {link.get('text', 'No title')[:50]}...")
        
        if len(final_links) > 5:
            print(f"   ... ë° ì¶”ê°€ {len(final_links) - 5}ê°œ ë§í¬")
        
        return final_links
    
    async def extract_content(self, page: Page, structure: Dict[str, Any]) -> Dict[str, Any]:
        """í–¥ìƒëœ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            content_selector = structure.get('contentSelector')
            page_url = page.url
            site_type = structure.get('siteType', {})

            # ë” ìŠ¤ë§ˆíŠ¸í•œ ì½˜í…ì¸  ì¶”ì¶œ
            content_data = await page.evaluate(f'''(selector, siteType) => {{
                let mainElement = null;
                
                // 1. ì‚¬ì´íŠ¸ë³„ ë§ì¶¤ ì„ íƒì ì‹œë„
                if (siteType.cms === 'wordpress') {{
                    const wpSelectors = ['.entry-content', '.post-content', 'article .content'];
                    for (const sel of wpSelectors) {{
                        mainElement = document.querySelector(sel);
                        if (mainElement) break;
                    }}
                }} else if (siteType.cms === 'medium') {{
                    const mediumSelectors = ['article section', '.postArticle-content'];
                    for (const sel of mediumSelectors) {{
                        mainElement = document.querySelector(sel);
                        if (mainElement) break;
                    }}
                }}
                
                // 2. ì œê³µëœ ì„ íƒì ì‹œë„
                if (!mainElement && selector) {{
                    const selectors = selector.split(',').map(s => s.trim());
                    for (const sel of selectors) {{
                        try {{
                            mainElement = document.querySelector(sel);
                            if (mainElement) break;
                        }} catch (e) {{}}
                    }}
                }}
                
                // 3. ì‹œë§¨í‹± íƒœê·¸ ì‹œë„
                if (!mainElement) {{
                    const semanticSelectors = ['article', 'main', '[role="main"]', '[role="article"]'];
                    for (const sel of semanticSelectors) {{
                        mainElement = document.querySelector(sel);
                        if (mainElement) break;
                    }}
                }}
                
                // 4. í…ìŠ¤íŠ¸ ë°€ë„ ê¸°ë°˜ ìë™ ê°ì§€
                if (!mainElement) {{
                    let bestElement = null;
                    let maxScore = 0;
                    
                    const candidates = document.querySelectorAll('div, section, article');
                    candidates.forEach(el => {{
                        if (el.offsetHeight === 0) return; // ìˆ¨ê²¨ì§„ ìš”ì†Œ ì œì™¸
                        
                        const textLength = (el.textContent || '').length;
                        const linkCount = el.querySelectorAll('a').length;
                        const imgCount = el.querySelectorAll('img').length;
                        
                        // ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ ë§ìŒ, ë§í¬ ì ìŒì´ ì¢‹ìŒ)
                        let score = textLength;
                        score -= linkCount * 50; // ë§í¬ê°€ ë§ìœ¼ë©´ ê°ì 
                        score += imgCount * 20;  // ì´ë¯¸ì§€ëŠ” ì•½ê°„ ê°€ì 
                        
                        // í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ê°€ì /ê°ì 
                        const className = el.className.toLowerCase();
                        if (className.includes('content') || className.includes('article') || className.includes('post')) {{
                            score += 100;
                        }}
                        if (className.includes('sidebar') || className.includes('nav') || className.includes('footer') || className.includes('header')) {{
                            score -= 200;
                        }}
                        
                        if (score > maxScore && textLength > 200) {{
                            maxScore = score;
                            bestElement = el;
                        }}
                    }});
                    
                    mainElement = bestElement;
                }}
                
                // 5. ìµœí›„ì˜ ìˆ˜ë‹¨
                if (!mainElement) {{
                    mainElement = document.body;
                }}
                
                return {{
                    html: mainElement ? mainElement.innerHTML : '',
                    text: mainElement ? mainElement.textContent : '',
                    selector_used: mainElement ? mainElement.tagName + (mainElement.className ? '.' + mainElement.className.split(' ')[0] : '') : 'body'
                }};
            }}''', content_selector, site_type)
            
            if not content_data['html']:
                return {'text': '', 'html': '', 'images': [], 'videos': [], 'date': ''}
            
            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹± ë° ì •ë¦¬
            soup = BeautifulSoup(content_data['html'], 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° (í™•ì¥ëœ ëª©ë¡)
            selectors_to_remove = [
                'script', 'style', 'nav', 'footer', 'header', 'aside',
                '.sidebar', '.nav', '.footer', '.menu', '.navigation',
                '.comments', '.comment-section', '.reply-form', '.comment-form',
                '.related-posts', '.related-articles', '.related-content',
                '.advertisement', '.ads', '.ad-container', '.ad-banner',
                '.social-media', '.share-buttons', '.social-share',
                '.author-bio', '.author-box', '.author-info',
                '.tags', '.categories', '.meta', '.breadcrumb',
                '.popup', '.modal', '.overlay', '.newsletter',
                'form', 'input', 'textarea', 'button', 'select',
                '[aria-hidden="true"]', '.hidden', '[style*="display:none"]',
                '.cookie-banner', '.gdpr', '.newsletter-signup'
            ]
            
            for selector in selectors_to_remove:
                for tag in soup.select(selector):
                    tag.decompose()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await self.extract_metadata(page)
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ê°œì„ ëœ í•„í„°ë§)
            image_urls = self.extract_media_urls(soup, page_url, 'img')
            
            # ë¹„ë””ì˜¤ URL ì¶”ì¶œ
            video_urls = self.extract_media_urls(soup, page_url, 'video')
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            text_content = soup.get_text(separator='\n').strip()
            text_content = re.sub(r'\n{3,}', '\n\n', text_content)
            text_content = re.sub(r'\s{3,}', ' ', text_content)
            
            print(f"ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ: {len(text_content)}ì, ì´ë¯¸ì§€ {len(image_urls)}ê°œ, ë¹„ë””ì˜¤ {len(video_urls)}ê°œ")
            
            return {
                'text': text_content,
                'html': str(soup),
                'images': image_urls,
                'videos': video_urls,
                'date': metadata['date'],
                'metadata': metadata,
                'selector_used': content_data['selector_used']
            }
            
        except Exception as e:
            print(f"ì½˜í…ì¸  ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {'text': '', 'html': '', 'images': [], 'videos': [], 'date': ''}
    
    def extract_media_urls(self, soup: BeautifulSoup, page_url: str, media_type: str) -> List[str]:
        """ë¯¸ë””ì–´ URL ì¶”ì¶œ (ì´ë¯¸ì§€/ë¹„ë””ì˜¤)"""
        media_urls = []
        
        try:
            if media_type == 'img':
                # ì´ë¯¸ì§€ ì¶”ì¶œ
                for img in soup.select('img[src]'):
                    src = img.get('src')
                    if not src:
                        continue
                    
                    # URL ì •ê·œí™”
                    src = urllib.parse.urljoin(page_url, src)
                    
                    # í¬ê¸° ë° í’ˆì§ˆ í•„í„°ë§
                    width = img.get('width')
                    height = img.get('height')
                    alt = img.get('alt', '').lower()
                    class_name = ' '.join(img.get('class', [])).lower()
                    
                    # ì‘ì€ ì´ë¯¸ì§€ ì œì™¸
                    try:
                        if width and int(width) < 150: continue
                        if height and int(height) < 150: continue
                    except (ValueError, TypeError):
                        pass
                    
                    # ì•„ì´ì½˜, ë¡œê³ , í”„ë¡œí•„ ì´ë¯¸ì§€ ì œì™¸
                    exclude_keywords = ['icon', 'logo', 'avatar', 'profile', 'badge', 'button', 'emoji']
                    if any(keyword in alt or keyword in class_name for keyword in exclude_keywords):
                        continue
                    
                    # ë°ì´í„° URL ì œì™¸
                    if src.startswith('data:'):
                        continue
                    
                    media_urls.append(src)
                    
            elif media_type == 'video':
                # ë¹„ë””ì˜¤ ì¶”ì¶œ
                # iframe ë¹„ë””ì˜¤ (YouTube, Vimeo ë“±)
                for iframe in soup.select('iframe[src]'):
                    src = iframe.get('src')
                    if not src:
                        continue
                    
                    src = urllib.parse.urljoin(page_url, src)
                    
                    # ë¹„ë””ì˜¤ ì„œë¹„ìŠ¤ ê°ì§€
                    video_domains = ['youtube.com', 'youtu.be', 'vimeo.com', 'dailymotion.com', 
                                   'player.twitch.tv', 'ted.com', 'wistia.com', 'brightcove.com']
                    
                    if any(domain in src.lower() for domain in video_domains):
                        media_urls.append(src)
                
                # video íƒœê·¸
                for video in soup.select('video'):
                    src = video.get('src')
                    if src:
                        media_urls.append(urllib.parse.urljoin(page_url, src))
                    
                    # source íƒœê·¸
                    for source in video.select('source[src]'):
                        src = source.get('src')
                        if src:
                            media_urls.append(urllib.parse.urljoin(page_url, src))
        
        except Exception as e:
            print(f"ë¯¸ë””ì–´ URL ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ìœ íš¨í•œ URLë§Œ ë°˜í™˜
        return list(set([url for url in media_urls if url and url.startswith('http')]))
    
    async def extract_metadata(self, page: Page) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            metadata = await page.evaluate(r'''() => {
                const meta = {};
                
                // ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                let dateStr = '';
                const dateSelectors = [
                    'meta[property="article:published_time"]',
                    'meta[name="date"]',
                    'meta[name="publish-date"]',
                    'meta[name="publication-date"]',
                    'time[datetime]',
                    'time',
                    '.date', '.published', '.timestamp',
                    '.post-date', '.article-date', '.entry-date'
                ];
                
                for (const selector of dateSelectors) {
                    const el = document.querySelector(selector);
                    if (el) {
                        const content = el.getAttribute('content') || 
                                       el.getAttribute('datetime') || 
                                       el.textContent;
                        if (content && content.trim()) {
                            dateStr = content.trim();
                            break;
                        }
                    }
                }
                
                meta.date = dateStr;
                
                // ê¸°íƒ€ ë©”íƒ€ë°ì´í„°
                const ogTitle = document.querySelector('meta[property="og:title"]');
                meta.og_title = ogTitle ? ogTitle.content : '';
                
                const ogDescription = document.querySelector('meta[property="og:description"]');
                meta.og_description = ogDescription ? ogDescription.content : '';
                
                const author = document.querySelector('meta[name="author"]') || 
                              document.querySelector('meta[property="article:author"]');
                meta.author = author ? author.content : '';
                
                const keywords = document.querySelector('meta[name="keywords"]');
                meta.keywords = keywords ? keywords.content : '';
                
                return meta;
            }''')
            
            return metadata
            
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {'date': ''}
    
    async def extract_headline(self, page: Page) -> str:
        """í–¥ìƒëœ ì œëª© ì¶”ì¶œ"""
        try:
            headline = await page.evaluate(r'''() => {
                // ìš°ì„ ìˆœìœ„: og:title > h1 > title
                const ogTitle = document.querySelector('meta[property="og:title"]');
                if (ogTitle && ogTitle.content && ogTitle.content.trim().length > 5) {
                    return ogTitle.content.trim();
                }
                
                const twitterTitle = document.querySelector('meta[name="twitter:title"]');
                if (twitterTitle && twitterTitle.content && twitterTitle.content.trim().length > 5) {
                    return twitterTitle.content.trim();
                }

                // ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ì—ì„œ h1 ì°¾ê¸°
                const contentAreas = ['article', 'main', '.content', '.post', '.entry'];
                for (const area of contentAreas) {
                    const container = document.querySelector(area);
                    if (container) {
                        const h1 = container.querySelector('h1');
                        if (h1 && h1.textContent && h1.textContent.trim().length > 5) {
                            return h1.textContent.trim();
                        }
                    }
                }
                
                // ì¼ë°˜ h1
                const h1 = document.querySelector('h1');
                if (h1 && h1.textContent) {
                    const h1Text = h1.textContent.trim();
                    if (h1Text.length > 5 && h1Text.length < 200) {
                        return h1Text;
                    }
                }
                
                // title íƒœê·¸ì—ì„œ ì‚¬ì´íŠ¸ëª… ì œê±°
                const titleTag = document.querySelector('title');
                if (titleTag && titleTag.textContent) {
                    let titleText = titleTag.textContent.trim();
                    
                    // ì¼ë°˜ì ì¸ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
                    const separators = ['|', '-', 'â€“', 'â€”', ':', 'Â»', 'Â«'];
                    for (const sep of separators) {
                        if (titleText.includes(sep)) {
                            const parts = titleText.split(sep).map(p => p.trim());
                            // ê°€ì¥ ê¸´ ë¶€ë¶„ì„ ì œëª©ìœ¼ë¡œ ì„ íƒ
                            parts.sort((a, b) => b.length - a.length);
                            if (parts[0] && parts[0].length > 5) {
                                titleText = parts[0];
                                break;
                            }
                        }
                    }
                    
                    if (titleText.length > 5 && titleText.length < 200) {
                        return titleText;
                    }
                }
                
                return '';
            }''')
            
            return headline
            
        except Exception as e:
            print(f"ì œëª© ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return ""
    
    async def crawl_url(self, url: str, content_focus: Optional[str] = None, max_links: int = 1, timeframe_hours_for_filter: int = 48) -> CrawlResult:
        """ë©”ì¸ í¬ë¡¤ë§ ë©”ì„œë“œ (ê°œì„ ë¨)"""
        result = CrawlResult(url)
        page = None
        
        try:
            page = await self.context.new_page()
            print(f"\n{'='*60}")
            print(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
            print(f"ì„¤ì • - ìµœëŒ€ ë§í¬: {max_links}, ì‹œê°„ ë²”ìœ„: {timeframe_hours_for_filter}ì‹œê°„")
            print(f"{'='*60}")
            
            # í˜ì´ì§€ ë¡œë“œ (ë” ê´€ëŒ€í•œ ì˜¤ë¥˜ ì²˜ë¦¬)
            try:
                await page.goto(url, wait_until='domcontentloaded', timeout=90000)
                print("âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ")
            except Exception as nav_error:
                print(f"âš ï¸ ì´ˆê¸° ë¡œë“œ ì‹¤íŒ¨, ì¬ì‹œë„: {nav_error}")
                try:
                    await page.goto(url, wait_until='load', timeout=60000)
                    print("âœ… ì¬ì‹œë„ ë¡œë“œ ì„±ê³µ")
                except Exception as final_error:
                    print(f"âŒ í˜ì´ì§€ ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {final_error}")
                    result.error = f"Navigation error: {final_error}"
                    return result
            
            # ì´ˆê¸° ë Œë”ë§ ëŒ€ê¸°
            await page.wait_for_timeout(3000)
            
            # íŒì—… ë° ì˜¤ë²„ë ˆì´ ì²˜ë¦¬
            await self.handle_popups_and_overlays(page)
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
            await page.wait_for_timeout(2000)
            
            # URL ì •ê·œí™”
            parsed_url = urllib.parse.urlparse(url)
            self.base_host = f"{parsed_url.scheme}://{parsed_url.netloc}"
            print(f"Base host ì„¤ì •: {self.base_host}")

            # í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
            print("ğŸ” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì¤‘...")
            structure = await self.analyze_page_structure(page, url)
            
            if not structure['links']:
                print("âš ï¸ ê²½ê³ : í˜ì´ì§€ì—ì„œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                result.error = "No links found on the page"
                return result

            # ì‚¬ì´íŠ¸ ì •ë³´ ì €ì¥
            result.site_info = structure['structure'].get('siteType', {})

            # ë§í¬ í•„í„°ë§
            print("ğŸ”— ë§í¬ í•„í„°ë§ ì¤‘...")
            links = await self.filter_relevant_links(structure['links'], content_focus)
            
            if not links:
                print("âš ï¸ ê²½ê³ : ê´€ë ¨ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                result.error = "No relevant links found after filtering"
                return result

            # ì²˜ë¦¬í•  ë§í¬ ì„ íƒ
            actual_max_links = min(len(links), max_links)
            links_to_crawl = links[:actual_max_links]
            
            print(f"\nğŸ“„ ì²˜ë¦¬í•  ë§í¬ {len(links_to_crawl)}ê°œ:")
            for i, link in enumerate(links_to_crawl, 1):
                print(f"  {i}. [{link.get('quality_score', 0)}ì ] {link.get('text', 'No title')[:60]}...")
                print(f"     ğŸ“ {link.get('href', 'No URL')}")

            # ê° ë§í¬ ì²˜ë¦¬
            for i, link_info in enumerate(links_to_crawl, 1):
                link_url = link_info.get('href')
                if not link_url:
                    print(f"âš ï¸ ë§í¬ {i}: URL ì—†ìŒ, ê±´ë„ˆëœ€")
                    continue

                print(f"\nğŸ”„ [{i}/{len(links_to_crawl)}] ë§í¬ ì²˜ë¦¬ ì¤‘...")
                print(f"ğŸ“ URL: {link_url}")
                
                link_page = None
                try:
                    link_page = await self.context.new_page()
                    
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if not link_url.startswith(('http://', 'https://')):
                        link_url = urllib.parse.urljoin(self.base_host, link_url)
                        print(f"ğŸ”— URL ì •ê·œí™”: {link_url}")
                    
                    # ë§í¬ í˜ì´ì§€ ë¡œë“œ
                    try:
                        await link_page.goto(link_url, wait_until='domcontentloaded', timeout=60000)
                        await link_page.wait_for_timeout(2000)
                    except Exception as e:
                        print(f"âŒ ë§í¬ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue

                    # íŒì—… ì²˜ë¦¬
                    await self.handle_popups_and_overlays(link_page)
                    
                    # ì½˜í…ì¸  ë¶„ì„ ë° ì¶”ì¶œ
                    print("ğŸ“„ ì½˜í…ì¸  ë¶„ì„ ì¤‘...")
                    link_structure = await self.analyze_page_structure(link_page, link_url)
                    content = await self.extract_content(link_page, link_structure['structure'])
                    headline = await self.extract_headline(link_page)
                    
                    # ë‚ ì§œ ì •ë³´ í™•ì¸
                    date_str = link_info.get('date', '') or content.get('date', '')
                    
                    # ë‚ ì§œ ê´€ë ¨ì„± í™•ì¸
                    if date_str:
                        date_is_relevant = is_relevant_date(date_str, self.target_date, timeframe_hours_for_filter)
                        if not date_is_relevant:
                            print(f"â° ë‚ ì§œ í•„í„°ë§: '{headline[:50]}...' ({date_str}) - {timeframe_hours_for_filter}ì‹œê°„ ì´ˆê³¼")
                            continue
                    
                    # ì½˜í…ì¸  í’ˆì§ˆ í™•ì¸ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
                    content_length = len(content.get('text', ''))
                    has_headline = bool(headline and headline.strip())
                    
                    if not has_headline and content_length < 100:  # ê¸°ì¤€ì„ 200ìì—ì„œ 100ìë¡œ ë‚®ì¶¤
                        print(f"âš ï¸ ì½˜í…ì¸  í’ˆì§ˆ ë¶€ì¡±: ì œëª© ì—†ìŒ, ë³¸ë¬¸ {content_length}ì")
                        print(f"   ë§í¬: {link_url}")
                        continue
                    
                    # ì œëª©ì´ ìˆê±°ë‚˜ ë³¸ë¬¸ì´ ì¶©ë¶„í•˜ë©´ í¬í•¨
                    if not has_headline:
                        print(f"âš ï¸ ì œëª© ì—†ì§€ë§Œ ë³¸ë¬¸ ì¶©ë¶„({content_length}ì) - í¬í•¨")
                    
                    if content_length < 100:
                        print(f"âš ï¸ ë³¸ë¬¸ ì§§ì§€ë§Œ({content_length}ì) ì œëª© ìˆìŒ - í¬í•¨")

                    # Story ê°ì²´ ìƒì„±
                    story = Story()
                    story.headline = headline or link_info.get('text', 'ì œëª© ì—†ìŒ')
                    story.link = link_url
                    story.date_posted = parse_date(date_str) if date_str else ''
                    story.fullContent = content.get('text', '')
                    story.imageUrls = content.get('images', [])
                    story.videoUrls = content.get('videos', [])
                    story.source = self._extract_domain_name(url)  # ì†ŒìŠ¤ ì‚¬ì´íŠ¸ ì •ë³´ ì¶”ê°€
                    
                    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                    metadata = content.get('metadata', {})
                    if metadata.get('keywords'):
                        story.tags = [tag.strip() for tag in metadata['keywords'].split(',') if tag.strip()]
                    
                    result.stories.append(story)
                    print(f"âœ… ì½˜í…ì¸  ì¶”ì¶œ ì„±ê³µ: '{story.headline[:50]}...' (ì¶œì²˜: {story.source})")
                    
                except Exception as e:
                    print(f"âŒ ë§í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                finally:
                    if link_page and not link_page.is_closed():
                        await link_page.close()
                    
                    # ìš”ì²­ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    if i < len(links_to_crawl):
                        await asyncio.sleep(2)
            
            print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(result.stories)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            
        except Exception as e:
            result.error = str(e)
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        finally:
            if page and not page.is_closed():
                await page.close()
        
        return result
    
    async def crawl_url_targeted(self, url: str, content_focus: Optional[str] = None, max_links: int = 1, timeframe_hours_for_filter: int = 48) -> CrawlResult:
        """ì‚¬ì´íŠ¸ë³„ íŠ¹í™”ëœ í¬ë¡¤ë§ ë©”ì„œë“œ"""
        result = CrawlResult(url)
        page = None
        
        try:
            # ì‚¬ì´íŠ¸ë³„ ë„ë©”ì¸ í™•ì¸
            parsed_url = urllib.parse.urlparse(url)
            domain = parsed_url.netloc.lower()
            
            print(f"\nğŸ¯ íƒ€ê²Ÿ í¬ë¡¤ë§ ì‹œì‘: {domain}")
            print(f"ğŸ“‹ ì„¤ì • - ìµœëŒ€ ë§í¬: {max_links}, ì‹œê°„ ë²”ìœ„: {timeframe_hours_for_filter}ì‹œê°„")
            
            # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” í¬ë¡¤ë§ ì‹¤í–‰
            if 'simonwillison.net' in domain:
                return await self._crawl_simon_willison(url, max_links)
            elif 'news.ycombinator.com' in domain:
                return await self._crawl_hacker_news(url, max_links)
            elif 'deepmind.google' in domain:
                return await self._crawl_deepmind_blog(url, max_links)
            elif 'mindstream.news' in domain:
                return await self._crawl_mindstream_news(url, max_links)
            elif 'aichief.com' in domain:
                return await self._crawl_aichief_news(url, max_links)
            else:
                # ì¼ë°˜ í¬ë¡¤ë§ìœ¼ë¡œ í´ë°±
                print(f"âš ï¸ ì•Œë ¤ì§€ì§€ ì•Šì€ ì‚¬ì´íŠ¸, ì¼ë°˜ í¬ë¡¤ë§ ì‚¬ìš©: {domain}")
                return await self.crawl_url(url, content_focus, max_links, timeframe_hours_for_filter)
                
        except Exception as e:
            result.error = str(e)
            print(f"âŒ íƒ€ê²Ÿ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        
        return result
    
    async def _crawl_simon_willison(self, url: str, max_links: int = 1) -> CrawlResult:
        """Simon Willison ë¸”ë¡œê·¸ íŠ¹í™” í¬ë¡¤ë§"""
        result = CrawlResult(url)
        page = None
        
        try:
            page = await self.context.new_page()
            print("ğŸ“– Simon Willison ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘")
            
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(2000)
            
            # ìµœì‹  ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì§ì ‘ ì°¾ê¸°
            blog_posts = await page.evaluate(r'''() => {
                const posts = [];
                
                // ë©”ì¸ í˜ì´ì§€ì—ì„œ ìµœì‹  í¬ìŠ¤íŠ¸ë“¤ ì°¾ê¸°
                const postSelectors = [
                    'article h2 a',
                    'h2 a[href*="/20"]',
                    '.entry-title a',
                    'a[href*="/2024/"], a[href*="/2025/"]'
                ];
                
                for (const selector of postSelectors) {
                    const links = document.querySelectorAll(selector);
                    for (const link of links) {
                        const href = link.href;
                        const title = link.textContent.trim();
                        
                        if (href && title && href.includes('/20') && title.length > 10) {
                            posts.push({
                                url: href,
                                title: title,
                                selector: selector
                            });
                        }
                    }
                }
                
                return posts.slice(0, 5); // ìµœëŒ€ 5ê°œ
            }''');
            
            print(f"ğŸ” ë°œê²¬ëœ í¬ìŠ¤íŠ¸: {len(blog_posts)}ê°œ")
            
            for i, post in enumerate(blog_posts[:max_links]):
                try:
                    print(f"ğŸ“„ í¬ìŠ¤íŠ¸ {i+1}: {post['title'][:50]}...")
                    
                    # ê°œë³„ í¬ìŠ¤íŠ¸ í˜ì´ì§€ ë°©ë¬¸
                    post_page = await self.context.new_page()
                    await post_page.goto(post['url'], wait_until='domcontentloaded', timeout=45000)
                    await post_page.wait_for_timeout(1500)
                    
                    # í¬ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                    content_data = await post_page.evaluate(r'''() => {
                        const article = document.querySelector('article') || 
                                       document.querySelector('.entry-content') ||
                                       document.querySelector('#content');
                        
                        if (!article) return { text: '', date: '', title: '' };
                        
                        const title = document.querySelector('h1')?.textContent?.trim() ||
                                     document.title.split('|')[0].trim();
                        
                        const dateEl = document.querySelector('time') ||
                                      document.querySelector('.published') ||
                                      document.querySelector('abbr.published');
                        
                        const date = dateEl ? (dateEl.getAttribute('datetime') || dateEl.textContent) : '';
                        
                        return {
                            text: article.textContent.trim(),
                            html: article.innerHTML,
                            title: title,
                            date: date
                        };
                    }''');
                    
                    if content_data['text'] and len(content_data['text']) > 100:
                        story = Story()
                        story.headline = content_data['title'] or post['title']
                        story.link = post['url']
                        story.date_posted = parse_date(content_data['date']) if content_data['date'] else ''
                        story.fullContent = content_data['text']
                        story.imageUrls = []
                        story.videoUrls = []
                        story.source = "simonwillison.net"
                        
                        result.stories.append(story)
                        print(f"âœ… í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì„±ê³µ: '{story.headline[:50]}...' (ì¶œì²˜: {story.source})")
                    
                    await post_page.close()
                    
                except Exception as e:
                    print(f"âŒ í¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    if 'post_page' in locals():
                        await post_page.close()
            
            await page.close()
            print(f"ğŸ‰ Simon Willison í¬ë¡¤ë§ ì™„ë£Œ: {len(result.stories)}ê°œ í¬ìŠ¤íŠ¸")
            
        except Exception as e:
            result.error = str(e)
            print(f"âŒ Simon Willison í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            if page:
                await page.close()
        
        return result
    
    async def _crawl_hacker_news(self, url: str, max_links: int = 1) -> CrawlResult:
        """Hacker News íŠ¹í™” í¬ë¡¤ë§ (ì‹œê°„ ì •ë³´ ì¶”ì¶œ ê°œì„ )"""
        result = CrawlResult(url)
        page = None
        
        try:
            page = await self.context.new_page()
            print("ğŸ“° Hacker News í¬ë¡¤ë§ ì‹œì‘")
            
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(2000)
            
            # HN ë©”ì¸ í˜ì´ì§€ì—ì„œ ìµœì‹  ê¸°ì‚¬ë“¤ ì°¾ê¸° (ì‹œê°„ ì •ë³´ í¬í•¨)
            stories = await page.evaluate('''() => {
                const stories = [];
                const storyElements = document.querySelectorAll('.athing');
                
                console.log(`ğŸ” HN: ${storyElements.length}ê°œ ìŠ¤í† ë¦¬ ìš”ì†Œ ë°œê²¬`);
                
                for (let i = 0; i < Math.min(10, storyElements.length); i++) {
                    const story = storyElements[i];
                    const titleLink = story.querySelector('.titleline > a');
                    
                    // ë‹¤ìŒ siblingì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    const metaRow = story.nextElementSibling;
                    let scoreElement = null;
                    let ageElement = null;
                    let ageText = '';
                    
                    if (metaRow) {
                        scoreElement = metaRow.querySelector('.score');
                        // age ì •ë³´ëŠ” ì—¬ëŸ¬ ì„ íƒìë¡œ ì‹œë„
                        ageElement = metaRow.querySelector('.age') || 
                                    metaRow.querySelector('span.age') ||
                                    metaRow.querySelector('a[title*="ago"]') ||
                                    metaRow.querySelector('span[title*="ago"]');
                        
                        // age í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if (ageElement) {
                            ageText = ageElement.textContent.trim() || ageElement.getAttribute('title') || '';
                        }
                        
                        // ë°±ì—…: metaRow ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ "ago" íŒ¨í„´ ì°¾ê¸°
                        if (!ageText && metaRow.textContent) {
                            const ageMatch = metaRow.textContent.match(/(\d+\s+(hour|day|minute)s?\s+ago)/i);
                            if (ageMatch) {
                                ageText = ageMatch[0];
                            }
                        }
                    }
                    
                    if (titleLink) {
                        const storyData = {
                            title: titleLink.textContent.trim(),
                            url: titleLink.href,
                            score: scoreElement ? scoreElement.textContent : '0 points',
                            age: ageText,
                            id: story.id
                        };
                        
                        console.log(`HN ìŠ¤í† ë¦¬ ${i+1}: ${storyData.title.substring(0, 50)}... (${storyData.age || 'No age'})`);
                        stories.push(storyData);
                    }
                }
                
                return stories;
            }''');
            
            print(f"ğŸ” ë°œê²¬ëœ HN ìŠ¤í† ë¦¬: {len(stories)}ê°œ")
            
            # ìŠ¤í† ë¦¬ ì •ë³´ ì¶œë ¥ (ì‹œê°„ í¬í•¨)
            for i, hn_story in enumerate(stories[:5]):
                age_info = f" ({hn_story['age']})" if hn_story['age'] else " (ì‹œê°„ì •ë³´ì—†ìŒ)"
                print(f"   {i+1}. {hn_story['title'][:50]}...{age_info}")
            
            processed_count = 0
            for i, hn_story in enumerate(stories):
                if processed_count >= max_links:
                    break
                    
                try:
                    print(f"ğŸ“„ ìŠ¤í† ë¦¬ {processed_count+1}: {hn_story['title'][:50]}...")
                    print(f"â° ê²Œì‹œ ì‹œê°„: {hn_story['age'] or 'Unknown'}")
                    
                    # ì‹œê°„ ì •ë³´ë¡œ ìµœì‹ ì„± í™•ì¸ (48ì‹œê°„ ì´ë‚´ë§Œ)
                    if hn_story['age']:
                        age_text = hn_story['age'].lower()
                        # "X hours ago" ë˜ëŠ” "X days ago" íŒŒì‹±
                        is_recent = False
                        
                        if 'hour' in age_text or 'minute' in age_text:
                            # ì‹œê°„/ë¶„ ë‹¨ìœ„ë©´ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
                            is_recent = True
                        elif 'day' in age_text:
                            # ì¼ ë‹¨ìœ„ì¸ ê²½ìš° ìˆ«ì í™•ì¸
                            import re
                            day_match = re.search(r'(\d+)\s*day', age_text)
                            if day_match and int(day_match.group(1)) <= 2:  # 2ì¼ ì´ë‚´
                                is_recent = True
                        
                        if not is_recent:
                            print(f"â° 48ì‹œê°„ ì´ˆê³¼ ìŠ¤í† ë¦¬ ê±´ë„ˆëœ€: {hn_story['age']}")
                            continue
                    
                    # ì™¸ë¶€ ë§í¬ë¡œ ì´ë™í•˜ì—¬ ë‚´ìš© ì¶”ì¶œ
                    if hn_story['url'].startswith('http') and 'news.ycombinator.com' not in hn_story['url']:
                        story_page = await self.context.new_page()
                        await story_page.goto(hn_story['url'], wait_until='domcontentloaded', timeout=45000)
                        await story_page.wait_for_timeout(2000)
                        
                        # ì™¸ë¶€ ì‚¬ì´íŠ¸ ë‚´ìš© ì¶”ì¶œ
                        content_data = await story_page.evaluate('''() => {
                            const article = document.querySelector('article') ||
                                           document.querySelector('main') ||
                                           document.querySelector('.content') ||
                                           document.querySelector('#content') ||
                                           document.body;
                            
                            const title = document.querySelector('h1')?.textContent?.trim() ||
                                         document.title.split('|')[0].trim();
                            
                            return {
                                text: article ? article.textContent.trim() : '',
                                title: title
                            };
                        }''');
                        
                        if content_data['text'] and len(content_data['text']) > 200:
                            story = Story()
                            story.headline = content_data['title'] or hn_story['title']
                            story.link = hn_story['url']
                            # HN ì‹œê°„ ì •ë³´ë¥¼ ë‚ ì§œë¡œ ë³€í™˜
                            story.date_posted = parse_date(hn_story['age']) if hn_story['age'] else ''
                            story.fullContent = content_data['text'][:2000]  # ê¸¸ì´ ì œí•œ
                            story.popularity = hn_story['score']
                            story.imageUrls = []
                            story.videoUrls = []
                            story.source = "news.ycombinator.com"
                            
                            result.stories.append(story)
                            processed_count += 1
                            print(f"âœ… HN ìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì„±ê³µ: '{story.headline[:50]}...' (ì¶œì²˜: {story.source})")
                        
                        await story_page.close()
                    else:
                        # HN ë‚´ë¶€ ë§í¬ì¸ ê²½ìš° (Ask HN, Show HN ë“±)
                        print(f"ğŸ”— HN ë‚´ë¶€ ë§í¬: {hn_story['url']}")
                        
                        story = Story()
                        story.headline = hn_story['title']
                        story.link = hn_story['url']
                        story.date_posted = parse_date(hn_story['age']) if hn_story['age'] else ''
                        story.fullContent = f"Hacker News í† ë¡ : {hn_story['title']}"
                        story.popularity = hn_story['score']
                        story.imageUrls = []
                        story.videoUrls = []
                        story.source = "news.ycombinator.com"
                        
                        result.stories.append(story)
                        processed_count += 1
                        print(f"âœ… HN ë‚´ë¶€ ìŠ¤í† ë¦¬ ìˆ˜ì§‘: '{story.headline[:50]}...' (ì¶œì²˜: {story.source})")
                    
                except Exception as e:
                    print(f"âŒ HN ìŠ¤í† ë¦¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    if 'story_page' in locals():
                        await story_page.close()
            
            await page.close()
            print(f"ğŸ‰ Hacker News í¬ë¡¤ë§ ì™„ë£Œ: {len(result.stories)}ê°œ ìŠ¤í† ë¦¬")
            
        except Exception as e:
            result.error = str(e)
            print(f"âŒ Hacker News í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            if page:
                await page.close()
        
        return result
    
    async def _crawl_deepmind_blog(self, url: str, max_links: int = 1) -> CrawlResult:
        """DeepMind ë¸”ë¡œê·¸ íŠ¹í™” í¬ë¡¤ë§ - ì²« í˜ì´ì§€ ìµœì‹  ê¸°ì‚¬ ìš°ì„ """
        result = CrawlResult(url)
        page = None
        
        try:
            page = await self.context.new_page()
            print("ğŸ§  DeepMind ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘")
            
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(5000)  # ë™ì  ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ì—ì„œ ëª¨ë“  ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
            blog_posts = await page.evaluate('''() => {
                const posts = [];
                console.log('ğŸ” DeepMind í˜ì´ì§€ ë¶„ì„ ì‹œì‘...');
                
                // í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ ê²€ì‚¬
                const allLinks = Array.from(document.querySelectorAll('a'));
                console.log(`ì´ ${allLinks.length}ê°œ ë§í¬ ë°œê²¬`);
                
                // DeepMind ë¸”ë¡œê·¸ URL íŒ¨í„´ í•„í„°ë§
                const blogLinks = allLinks.filter(link => {
                    const href = link.href;
                    const hasText = link.textContent && link.textContent.trim().length > 15;
                    const isBlogPost = href && (
                        href.includes('/discover/blog/') ||
                        href.includes('/research/') ||
                        href.includes('/publications/')
                    );
                    return hasText && isBlogPost;
                });
                
                console.log(`í•„í„°ë§ í›„ ${blogLinks.length}ê°œ ë¸”ë¡œê·¸ ë§í¬`);
                
                // ê° ë§í¬ì˜ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                blogLinks.forEach((link, index) => {
                    try {
                        const href = link.href;
                        const title = link.textContent.trim();
                        
                        // ì»¨í…Œì´ë„ˆì—ì„œ ë‚ ì§œ ì •ë³´ ì°¾ê¸°
                        let dateStr = '';
                        let container = link.closest('article, .card, .post, .item, [role="listitem"]');
                        
                        if (container) {
                            // ì‹œê°„ ìš”ì†Œ ì°¾ê¸°
                            const timeEl = container.querySelector('time');
                            if (timeEl) {
                                dateStr = timeEl.getAttribute('datetime') || timeEl.textContent;
                            }
                            
                            // ë‚ ì§œ íŒ¨í„´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
                            if (!dateStr) {
                                const containerText = container.textContent;
                                const dateMatch = containerText.match(/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},?\\s+\\d{4}|\\d{4}-\\d{2}-\\d{2}/i);
                                if (dateMatch) {
                                    dateStr = dateMatch[0];
                                }
                            }
                        }
                        
                        // DOM ìœ„ì¹˜ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ (ìƒë‹¨ì— ìˆì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
                        const rect = link.getBoundingClientRect();
                        const topPosition = rect.top + window.scrollY;
                        const priority = Math.max(0, 10000 - topPosition); // ìƒë‹¨ì— ìˆì„ìˆ˜ë¡ ë†’ì€ ìš°ì„ ìˆœìœ„
                        
                        posts.push({
                            url: href,
                            title: title,
                            date: dateStr,
                            priority: priority,
                            position: index
                        });
                        
                        console.log(`ë§í¬ ${index + 1}: ${title.substring(0, 50)}... (ìœ„ì¹˜: ${topPosition}, ìš°ì„ ìˆœìœ„: ${priority})`);
                        
                    } catch (e) {
                        console.warn(`ë§í¬ ì²˜ë¦¬ ì˜¤ë¥˜:`, e);
                    }
                });
                
                // ìš°ì„ ìˆœìœ„ì™€ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (í˜ì´ì§€ ìƒë‹¨ + ì¸ë±ìŠ¤ ìˆœì„œ)
                posts.sort((a, b) => {
                    // 1ìˆœìœ„: ë‚ ì§œê°€ ìˆëŠ” ê²ƒ
                    if (a.date && !b.date) return -1;
                    if (!a.date && b.date) return 1;
                    
                    // 2ìˆœìœ„: ìš°ì„ ìˆœìœ„ (í˜ì´ì§€ ìƒë‹¨ ìœ„ì¹˜)
                    if (Math.abs(a.priority - b.priority) > 100) {
                        return b.priority - a.priority;
                    }
                    
                    // 3ìˆœìœ„: DOM ìˆœì„œ (ë¨¼ì € ë‚˜íƒ€ë‚˜ëŠ” ê²ƒ)
                    return a.position - b.position;
                });
                
                console.log('ì •ë ¬ëœ ìƒìœ„ 5ê°œ í¬ìŠ¤íŠ¸:');
                posts.slice(0, 5).forEach((post, i) => {
                    console.log(`${i + 1}. ${post.title.substring(0, 60)}... (ë‚ ì§œ: ${post.date || 'N/A'})`);
                });
                
                return posts.slice(0, 10);  // ìƒìœ„ 10ê°œ í›„ë³´
            }''');
            
            print(f"ğŸ” ë°œê²¬ëœ DeepMind í¬ìŠ¤íŠ¸: {len(blog_posts)}ê°œ")
            
            # ìƒìœ„ í›„ë³´ë“¤ ì¶œë ¥
            for i, post in enumerate(blog_posts[:5]):
                print(f"   {i+1}. {post['title'][:60]}... ({post.get('date', 'No date')})")
            
            processed_count = 0
            for i, post in enumerate(blog_posts):
                if processed_count >= max_links:
                    break
                    
                try:
                    print(f"ğŸ“„ í¬ìŠ¤íŠ¸ {processed_count+1}: {post['title'][:50]}...")
                    
                    post_page = await self.context.new_page()
                    await post_page.goto(post['url'], wait_until='domcontentloaded', timeout=45000)
                    await post_page.wait_for_timeout(2000)
                    
                    # í¬ìŠ¤íŠ¸ ë‚´ìš© ë° ë‚ ì§œ ì¶”ì¶œ
                    content_data = await post_page.evaluate('''() => {
                        const article = document.querySelector('article') ||
                                       document.querySelector('main') ||
                                       document.querySelector('.content') ||
                                       document.querySelector('#content') ||
                                       document.body;
                        
                        const title = document.querySelector('h1')?.textContent?.trim() ||
                                     document.title.split('|')[0].trim();
                        
                        // ë” ì ê·¹ì ì¸ ë‚ ì§œ ì°¾ê¸°
                        let date = '';
                        const dateSelectors = [
                            'time[datetime]',
                            'meta[property="article:published_time"]',
                            'meta[name="date"]',
                            '.date',
                            '.published',
                            '.timestamp',
                            '.post-date',
                            '.byline time'
                        ];
                        
                        for (const selector of dateSelectors) {
                            const el = document.querySelector(selector);
                            if (el) {
                                date = el.getAttribute('datetime') || 
                                       el.getAttribute('content') || 
                                       el.textContent;
                                if (date && date.trim()) {
                                    date = date.trim();
                                    break;
                                }
                            }
                        }
                        
                        // í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (ë°±ì—…)
                        if (!date) {
                            const bodyText = document.body.textContent;
                            const dateMatch = bodyText.match(/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},?\\s+\\d{4}|\\d{4}-\\d{2}-\\d{2}/i);
                            if (dateMatch) {
                                date = dateMatch[0];
                            }
                        }
                        
                        return {
                            text: article ? article.textContent.trim() : '',
                            title: title,
                            date: date
                        };
                    }''');
                    
                    if content_data['text'] and len(content_data['text']) > 200:
                        # ë‚ ì§œ ìœ íš¨ì„± í™•ì¸
                        final_date = content_data['date'] or post.get('date', '')
                        parsed_date = parse_date(final_date) if final_date else ''
                        
                        # 48ì‹œê°„ ë‚´ ê¸°ì‚¬ì¸ì§€ í™•ì¸
                        if parsed_date:
                            is_recent = is_relevant_date(parsed_date, None, 48)
                            if not is_recent:
                                print(f"â° 48ì‹œê°„ ì´ˆê³¼: {post['title'][:50]}... ({parsed_date})")
                                await post_page.close()
                                continue
                        
                        story = Story()
                        story.headline = content_data['title'] or post['title']
                        story.link = post['url']
                        story.date_posted = parsed_date
                        story.fullContent = content_data['text']
                        story.imageUrls = []
                        story.videoUrls = []
                        story.source = "deepmind.google"
                        
                        result.stories.append(story)
                        processed_count += 1
                        print(f"âœ… DeepMind í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì„±ê³µ: '{story.headline[:50]}...' (ì¶œì²˜: {story.source})")
                    else:
                        print(f"âš ï¸ ì½˜í…ì¸  ë¶€ì¡±: {len(content_data.get('text', ''))}ì")
                    
                    await post_page.close()
                    
                except Exception as e:
                    print(f"âŒ DeepMind í¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    if 'post_page' in locals():
                        await post_page.close()
            
            await page.close()
            print(f"ğŸ‰ DeepMind í¬ë¡¤ë§ ì™„ë£Œ: {len(result.stories)}ê°œ í¬ìŠ¤íŠ¸")
            
        except Exception as e:
            result.error = str(e)
            print(f"âŒ DeepMind í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            if page:
                await page.close()
        
        return result
    
    async def _crawl_mindstream_news(self, url: str, max_links: int = 1) -> CrawlResult:
        """Mindstream News íŠ¹í™” í¬ë¡¤ë§ (ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ê¸°ë°˜ ìµœì í™”)"""
        result = CrawlResult(url)
        page = None
        
        try:
            page = await self.context.new_page()
            print("ğŸ“º Mindstream News í¬ë¡¤ë§ ì‹œì‘")
            
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(3000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ì¿ í‚¤ íŒì—… ì²˜ë¦¬
            await self.handle_popups_and_overlays(page)
            
            # Mindstream Archive ì‹¤ì œ êµ¬ì¡° ë¶„ì„ ë° ìµœì í™”ëœ ì¶”ì¶œ
            articles = await page.evaluate(r'''() => {
                console.log('ğŸ” Mindstream Archive êµ¬ì¡° ë¶„ì„ ì‹œì‘...');
                
                const articles = [];
                const processedUrls = new Set();
                
                // ì „ì²´ í˜ì´ì§€ì—ì„œ ë‚ ì§œ íŒ¨í„´ ë¨¼ì € ì°¾ê¸°
                const pageText = document.body.textContent || '';
                const allDateMatches = [...pageText.matchAll(/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}/gi)];
                console.log(`ğŸ“… í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ë‚ ì§œ íŒ¨í„´: ${allDateMatches.length}ê°œ`);
                allDateMatches.forEach(match => console.log(`   ë‚ ì§œ: ${match[0]}`));
                
                // Mindstream /p/ ë§í¬ë“¤ ì°¾ê¸°
                const articleLinks = document.querySelectorAll('a[href*="/p/"]');
                console.log(`ğŸ” /p/ íŒ¨í„´ ë§í¬ ${articleLinks.length}ê°œ ë°œê²¬`);
                
                articleLinks.forEach((link, index) => {
                    const href = link.href;
                    let title = link.textContent.trim();
                    
                    // ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    if (!href || !title || title.length < 10) {
                        console.log(`âš ï¸ ë§í¬ ${index + 1} ê±´ë„ˆë›°ê¸°: ì œëª© ë¶€ì¡± ("${title}")`);
                        return;
                    }
                    
                    if (processedUrls.has(href)) {
                        console.log(`âš ï¸ ë§í¬ ${index + 1} ê±´ë„ˆë›°ê¸°: ì¤‘ë³µ URL`);
                        return;
                    }
                    
                    processedUrls.add(href);
                    
                    // ë‚ ì§œ ì¶”ì¶œ ì „ëµ ê°œì„ 
                    let dateStr = '';
                    let searchRadius = 5; // íƒìƒ‰ ë°˜ê²½ í™•ëŒ€
                    
                    // ì „ëµ 1: ë§í¬ì™€ ê·¼ì ‘í•œ ìš”ì†Œë“¤ì—ì„œ ë‚ ì§œ ì°¾ê¸°
                    let currentElement = link;
                    for (let i = 0; i < searchRadius; i++) {
                        // ë¶€ëª¨ ìš”ì†Œë“¤ íƒìƒ‰
                        if (currentElement.parentElement) {
                            currentElement = currentElement.parentElement;
                            const elementText = currentElement.textContent || '';
                            const dateMatch = elementText.match(/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}/i);
                            if (dateMatch) {
                                dateStr = dateMatch[0];
                                console.log(`ğŸ“… ë¶€ëª¨ ${i+1}ì—ì„œ ë‚ ì§œ ë°œê²¬: "${dateStr}" for "${title.substring(0, 30)}..."`);
                                break;
                            }
                        }
                    }
                    
                    // ì „ëµ 2: ë§í¬ ì£¼ë³€ í˜•ì œ ìš”ì†Œë“¤ íƒìƒ‰
                    if (!dateStr) {
                        const parentElement = link.parentElement;
                        if (parentElement) {
                            const siblings = Array.from(parentElement.children);
                            for (const sibling of siblings) {
                                const siblingText = sibling.textContent || '';
                                const dateMatch = siblingText.match(/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}/i);
                                if (dateMatch) {
                                    dateStr = dateMatch[0];
                                    console.log(`ğŸ“… í˜•ì œ ìš”ì†Œì—ì„œ ë‚ ì§œ ë°œê²¬: "${dateStr}" for "${title.substring(0, 30)}..."`);
                                    break;
                                }
                            }
                        }
                    }
                    
                    // ì „ëµ 3: í˜ì´ì§€ ìƒë‹¨ë¶€í„° ìˆœì„œëŒ€ë¡œ ë§¤ì¹­ (ìµœì‹  ê¸°ì‚¬ì¼ìˆ˜ë¡ ìƒë‹¨ì— ìœ„ì¹˜)
                    if (!dateStr && allDateMatches.length > 0) {
                        // ë§í¬ì˜ ìˆœì„œì— ë”°ë¼ ë‚ ì§œ í• ë‹¹ (ìƒë‹¨ ë§í¬ = ìµœì‹  ë‚ ì§œ)
                        if (index < allDateMatches.length) {
                            dateStr = allDateMatches[index][0];
                            console.log(`ğŸ“… ìˆœì„œ ë§¤ì¹­ìœ¼ë¡œ ë‚ ì§œ í• ë‹¹: "${dateStr}" for "${title.substring(0, 30)}..." (ìœ„ì¹˜: ${index + 1})`);
                        } else {
                            // ë‚ ì§œê°€ ë¶€ì¡±í•œ ê²½ìš° ê°€ì¥ ìµœì‹  ë‚ ì§œ ì‚¬ìš©
                            dateStr = allDateMatches[0][0];
                            console.log(`ğŸ“… ìµœì‹  ë‚ ì§œë¡œ í´ë°±: "${dateStr}" for "${title.substring(0, 30)}..."`);
                        }
                    }
                    
                    // ì œëª© ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°)
                    title = title.replace(/\s+/g, ' ').trim();
                    
                    // DOM ìˆœì„œ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ (ìƒë‹¨ = ìµœì‹ )
                    const priority = 1000 - index;
                    
                    const article = {
                        url: href,
                        title: title,
                        date: dateStr,
                        priority: priority,
                        index: index,
                        source: 'mindstream'  // ì†ŒìŠ¤ ëª…ì‹œ
                    };
                    
                    articles.push(article);
                    console.log(`ğŸ“° ${index + 1}. "${title.substring(0, 50)}..." â†’ ë‚ ì§œ: ${dateStr || 'N/A'}`);
                });
                
                // ìš°ì„ ìˆœìœ„ ì •ë ¬ (ìµœì‹ ìˆœ ìœ ì§€)
                articles.sort((a, b) => {
                    // 1ìˆœìœ„: ë‚ ì§œê°€ ìˆëŠ” ê²ƒ ìš°ì„ 
                    if (a.date && !b.date) return -1;
                    if (!a.date && b.date) return 1;
                    
                    // 2ìˆœìœ„: ë‚ ì§œ ìµœì‹ ìˆœ
                    if (a.date && b.date) {
                        try {
                            const dateA = new Date(a.date);
                            const dateB = new Date(b.date);
                            return dateB - dateA; // ìµœì‹ ìˆœ
                        } catch (e) {
                            return b.date.localeCompare(a.date);
                        }
                    }
                    
                    // 3ìˆœìœ„: DOM ìˆœì„œ (ìƒë‹¨ ìš°ì„ )
                    return a.index - b.index;
                });
                
                console.log(`ğŸ“Š Mindstream ì´ ${articles.length}ê°œ ê¸°ì‚¬ ë°œê²¬`);
                console.log('ğŸ† ì •ë ¬ëœ ìƒìœ„ ê¸°ì‚¬ë“¤:');
                articles.slice(0, 5).forEach((article, i) => {
                    console.log(`   ${i + 1}. ${article.title.substring(0, 50)}... (${article.date || 'No date'})`);
                });
                
                return articles.slice(0, 10);  // ìƒìœ„ 10ê°œ
            }''');
            
            print(f"ğŸ” ë°œê²¬ëœ Mindstream ê¸°ì‚¬: {len(articles)}ê°œ")
            
            # ìƒìœ„ í›„ë³´ë“¤ ì¶œë ¥
            for i, article in enumerate(articles[:5]):
                print(f"   {i+1}. {article['title'][:60]}... ({article.get('date', 'No date')})")
            
            processed_count = 0
            for i, article in enumerate(articles):
                if processed_count >= max_links:
                    break
                    
                try:
                    print(f"ğŸ“„ ê¸°ì‚¬ {processed_count+1}: {article['title'][:50]}...")
                    print(f"ğŸ”— URL: {article['url']}")
                    
                    article_page = await self.context.new_page()
                    
                    try:
                        await article_page.goto(article['url'], wait_until='domcontentloaded', timeout=45000)
                        await article_page.wait_for_timeout(3000)
                        
                        # ì¿ í‚¤ íŒì—… ì²˜ë¦¬
                        await self.handle_popups_and_overlays(article_page)
                        
                        # Mindstream íŠ¹í™” ì½˜í…ì¸  ì¶”ì¶œ
                        content_data = await article_page.evaluate('''() => {
                            console.log('ğŸ” Mindstream ê¸°ì‚¬ ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘...');
                            
                            // Mindstream/Beehiiv í”Œë«í¼ íŠ¹í™” ì½˜í…ì¸  ì„ íƒì
                            const mindstreamSelectors = [
                                // Beehiiv/Mindstream ê¸°ë³¸ ì½˜í…ì¸  êµ¬ì¡°
                                'div[data-block-type="paragraph"]',  // Beehiiv ë‹¨ë½
                                'div[data-block-type="unstyled"]',   // Beehiiv ê¸°ë³¸ í…ìŠ¤íŠ¸
                                '.post-content',                     // ì¼ë°˜ì ì¸ í¬ìŠ¤íŠ¸ ì½˜í…ì¸ 
                                'article',                           // ì‹œë§¨í‹± article íƒœê·¸
                                'main',                              // ë©”ì¸ ì½˜í…ì¸ 
                                '.content',                          // ì½˜í…ì¸  í´ë˜ìŠ¤
                                '[role="main"]',                     // ARIA ë©”ì¸ ì—­í• 
                            ];
                            
                            let contentElement = null;
                            let contentText = '';
                            
                            // ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„
                            for (const selector of mindstreamSelectors) {
                                const elements = document.querySelectorAll(selector);
                                if (elements.length > 0) {
                                    // ì—¬ëŸ¬ ìš”ì†Œê°€ ìˆëŠ” ê²½ìš° ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
                                    const combinedText = Array.from(elements)
                                        .map(el => el.textContent.trim())
                                        .filter(text => text.length > 20)  // ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                                        .join('\\n\\n');
                                    
                                    if (combinedText.length > contentText.length) {
                                        contentText = combinedText;
                                        contentElement = elements[0];
                                        console.log(`âœ… ì½˜í…ì¸  ì„ íƒì ì„±ê³µ: ${selector} (${combinedText.length}ì)`);
                                    }
                                }
                            }
                            
                            // ë°±ì—…: í˜ì´ì§€ ì „ì²´ì—ì„œ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
                            if (!contentText || contentText.length < 200) {
                                console.log('âš ï¸ ê¸°ë³¸ ì„ íƒì ì‹¤íŒ¨, ë°±ì—… ë°©ì‹ ì‹œë„...');
                                
                                const allDivs = document.querySelectorAll('div, section, article');
                                let bestElement = null;
                                let bestScore = 0;
                                
                                allDivs.forEach(div => {
                                    const text = div.textContent.trim();
                                    const textLength = text.length;
                                    
                                    // í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
                                    if (textLength > 100 && textLength < 10000) {
                                        let score = textLength;
                                        
                                        // ë§í¬ ë¹„ìœ¨ë¡œ ê°ì 
                                        const linkCount = div.querySelectorAll('a').length;
                                        const linkPenalty = linkCount * 50;
                                        score -= linkPenalty;
                                        
                                        // ì¢‹ì€ ì½˜í…ì¸  ì‹ í˜¸ë¡œ ê°€ì 
                                        const className = div.className.toLowerCase();
                                        if (className.includes('content') || 
                                            className.includes('post') || 
                                            className.includes('article')) {
                                            score += 200;
                                        }
                                        
                                        if (score > bestScore) {
                                            bestScore = score;
                                            bestElement = div;
                                        }
                                    }
                                });
                                
                                if (bestElement) {
                                    contentText = bestElement.textContent.trim();
                                    contentElement = bestElement;
                                    console.log(`âœ… ë°±ì—… ì¶”ì¶œ ì„±ê³µ: ${contentText.length}ì`);
                                }
                            }
                            
                            // ì œëª© ì¶”ì¶œ (Mindstream íŠ¹í™”)
                            let title = '';
                            const titleSelectors = [
                                'h1',                               // ë©”ì¸ ì œëª©
                                'meta[property="og:title"]',        // OpenGraph ì œëª©
                                'meta[name="twitter:title"]',       // Twitter ì œëª©
                                'title'                             // í˜ì´ì§€ ì œëª©
                            ];
                            
                            for (const selector of titleSelectors) {
                                const el = document.querySelector(selector);
                                if (el) {
                                    const candidateTitle = selector.includes('meta') ? 
                                                          el.getAttribute('content') : 
                                                          el.textContent;
                                    if (candidateTitle && candidateTitle.trim().length > 5) {
                                        title = candidateTitle.trim();
                                        // ì‚¬ì´íŠ¸ëª… ì œê±° (Mindstream íŠ¹í™”)
                                        title = title.replace(/\\s*[-|â€“]\\s*Mindstream.*$/i, '');
                                        break;
                                    }
                                }
                            }
                            
                            // ë‚ ì§œ ì¶”ì¶œ (Mindstream/Beehiiv ê°•í™”)
                            let date = '';
                            
                            // 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„°ì—ì„œ ë‚ ì§œ ì°¾ê¸°
                            const metaSelectors = [
                                'meta[property="article:published_time"]',  // OpenGraph ë°œí–‰ì‹œê°„
                                'meta[name="date"]',                        // ë‚ ì§œ ë©”íƒ€
                                'meta[name="publish-date"]',                // ë°œí–‰ ë‚ ì§œ
                                'time[datetime]'                            // HTML5 time íƒœê·¸
                            ];
                            
                            for (const selector of metaSelectors) {
                                const el = document.querySelector(selector);
                                if (el) {
                                    const candidateDate = el.getAttribute('datetime') || 
                                                         el.getAttribute('content');
                                    if (candidateDate && candidateDate.trim()) {
                                        date = candidateDate.trim();
                                        console.log(`ğŸ“… ë©”íƒ€ë°ì´í„°ì—ì„œ ë‚ ì§œ ë°œê²¬: ${date}`);
                                        break;
                                    }
                                }
                            }
                            
                            // 2ë‹¨ê³„: í˜ì´ì§€ ë³¸ë¬¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (Mindstream íŠ¹í™”)
                            if (!date) {
                                const bodyText = document.body.textContent || '';
                                const datePatterns = [
                                    // Mindstreamì˜ "May 23, 2025" í˜•íƒœ
                                    /(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},\s+\d{4}/i,
                                    // ISO í˜•íƒœ
                                    /\d{4}-\d{2}-\d{2}/,
                                    // ë‹¤ë¥¸ í˜•íƒœë“¤
                                    /\d{1,2}\/\d{1,2}\/\d{4}/,
                                    /\d{1,2}\.\d{1,2}\.\d{4}/
                                ];
                                
                                for (const pattern of datePatterns) {
                                    const match = bodyText.match(pattern);
                                    if (match) {
                                        date = match[0];
                                        console.log(`ğŸ“… ë³¸ë¬¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ë°œê²¬: ${date}`);
                                        break;
                                    }
                                }
                            }
                            
                            // 3ë‹¨ê³„: DOM ìš”ì†Œì—ì„œ ë‚ ì§œ ì°¾ê¸°
                            if (!date) {
                                const domSelectors = [
                                    'time',                              // time íƒœê·¸
                                    '.date, .published, .timestamp',     // ë‚ ì§œ í´ë˜ìŠ¤
                                    '[class*="date"], [id*="date"]',     // ë‚ ì§œ í¬í•¨ í´ë˜ìŠ¤/ID
                                    'span[title*="2025"], span[title*="2024"]'  // íˆ´íŒì— ì—°ë„
                                ];
                                
                                for (const selector of domSelectors) {
                                    const elements = document.querySelectorAll(selector);
                                    for (const el of elements) {
                                        const candidateDate = el.getAttribute('title') ||
                                                             el.getAttribute('datetime') ||
                                                             el.textContent;
                                        if (candidateDate && candidateDate.trim()) {
                                            const cleanDate = candidateDate.trim();
                                            // ë‚ ì§œ íŒ¨í„´ ê²€ì¦
                                            if (/\d{4}|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec/i.test(cleanDate)) {
                                                date = cleanDate;
                                                console.log(`ğŸ“… DOM ìš”ì†Œì—ì„œ ë‚ ì§œ ë°œê²¬: ${date}`);
                                                break;
                                            }
                                        }
                                    }
                                    if (date) break;
                                }
                            }
                            
                            // ìµœì¢… ê²°ê³¼
                            const result = {
                                text: contentText,
                                title: title,
                                date: date,
                                contentLength: contentText.length,
                                method: contentElement ? 
                                       `${contentElement.tagName}${contentElement.className ? '.' + contentElement.className.split(' ')[0] : ''}` : 
                                       'text-analysis'
                            };
                            
                            console.log(`ğŸ“Š Mindstream ì¶”ì¶œ ê²°ê³¼: ${result.contentLength}ì, ì œëª©: "${result.title?.substring(0, 30)}..."`);
                            return result;
                        }''');
                        
                        # ì½˜í…ì¸  í’ˆì§ˆ í™•ì¸ (Mindstream ìµœì í™”)
                        content_length = len(content_data.get('text', ''))
                        has_title = bool(content_data.get('title', '').strip())
                        
                        if content_length > 100 or (has_title and content_length > 50):
                            # ë‚ ì§œ ê²€ì¦
                            final_date = content_data.get('date') or article.get('date', '')
                            parsed_date = parse_date(final_date) if final_date else ''
                            
                            # ì‹œê°„ í•„í„°ë§ (Mindstreamì€ ë§¤ìš° ê´€ëŒ€í•˜ê²Œ - 30ì¼)
                            if parsed_date:
                                is_recent = is_relevant_date(parsed_date, None, 720)  # 30ì¼ (720ì‹œê°„)
                                if not is_recent:
                                    print(f"â° 30ì¼ ì´ˆê³¼: {article['title'][:50]}... ({parsed_date})")
                                    await article_page.close()
                                    continue
                            else:
                                # ë‚ ì§œê°€ ì—†ì–´ë„ Mindstreamì€ í¬í•¨ (ìµœì‹  ê¸°ì‚¬ì¼ ê°€ëŠ¥ì„±)
                                print(f"ğŸ“… ë‚ ì§œ ì •ë³´ ì—†ìŒ, í•˜ì§€ë§Œ Mindstreamì´ë¯€ë¡œ í¬í•¨: {article['title'][:50]}...")
                            
                            # Story ê°ì²´ ìƒì„±
                            story = Story()
                            story.headline = content_data.get('title') or article['title']
                            story.link = article['url']
                            story.date_posted = parsed_date
                            story.fullContent = content_data['text']
                            story.imageUrls = []  # Mindstreamì€ ì£¼ë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜
                            story.videoUrls = []
                            story.source = "mindstream.news"
                            
                            # Mindstream íŠ¹í™” ë©”íƒ€ë°ì´í„°
                            story.summary = story.fullContent[:200] + "..." if len(story.fullContent) > 200 else story.fullContent
                            story.tags = ['mindstream', 'ai-news', 'tech-news']  # Mindstream ì‹ë³„ íƒœê·¸
                            
                            # ë‚ ì§œê°€ ìµœì‹ ì¸ ê²½ìš° ìš°ì„ ìˆœìœ„ í‘œì‹œ
                            if parsed_date:
                                try:
                                    from datetime import datetime
                                    date_obj = datetime.strptime(parsed_date, '%Y-%m-%d') if len(parsed_date) == 10 else datetime.now()
                                    days_old = (datetime.now() - date_obj).days
                                    if days_old <= 3:  # 3ì¼ ì´ë‚´
                                        story.popularity = f"fresh-{days_old}d"  # ì‹ ì„ ë„ í‘œì‹œ
                                except:
                                    pass
                            
                            result.stories.append(story)
                            processed_count += 1
                            
                            print(f"âœ… Mindstream ê¸°ì‚¬ ìˆ˜ì§‘ ì„±ê³µ: '{story.headline[:50]}...' (ì¶œì²˜: {story.source})")
                            
                        else:
                            print(f"âš ï¸ ì½˜í…ì¸  í’ˆì§ˆ ë¶€ì¡±:")
                            print(f"   ğŸ“Š í…ìŠ¤íŠ¸ ê¸¸ì´: {content_length}ì")
                            print(f"   ğŸ“ ì œëª© ì¡´ì¬: {'Yes' if has_title else 'No'}")
                            print(f"   ğŸ¯ ìµœì†Œ ìš”êµ¬ì‚¬í•­: ì œëª© ìˆìœ¼ë©´ 50ì+, ì—†ìœ¼ë©´ 100ì+")
                        
                    except Exception as page_error:
                        print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {page_error}")
                    finally:
                        await article_page.close()
                    
                except Exception as e:
                    print(f"âŒ Mindstream ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    if 'article_page' in locals() and not article_page.is_closed():
                        await article_page.close()
            
            await page.close()
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            print(f"ğŸ‰ Mindstream í¬ë¡¤ë§ ì™„ë£Œ: {len(result.stories)}ê°œ ê¸°ì‚¬")
            if result.stories:
                print("ğŸ“‹ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡:")
                for i, story in enumerate(result.stories, 1):
                    freshness = f" [{story.popularity}]" if story.popularity else ""
                    print(f"   {i}. {story.headline[:60]}...{freshness}")
                    print(f"      ğŸ“… ë‚ ì§œ: {story.date_posted or 'Unknown'}")
                    print(f"      ğŸ“Š ë‚´ìš©: {len(story.fullContent)}ì")
                    print(f"      ğŸ”— URL: {story.link}")
            else:
                print("âš ï¸ Mindstreamì—ì„œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                print("ğŸ” ê°€ëŠ¥í•œ ì›ì¸:")
                print("   â€¢ ì¿ í‚¤ íŒì—…ì´ ì œëŒ€ë¡œ ì²˜ë¦¬ë˜ì§€ ì•ŠìŒ")
                print("   â€¢ í˜ì´ì§€ ë¡œë”© ì‹œê°„ ë¶€ì¡±")
                print("   â€¢ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨")
                print("   â€¢ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨")
            
        except Exception as e:
            result.error = str(e)
            print(f"âŒ Mindstream í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            if page:
                await page.close()
        
        return result
    
    async def _crawl_aichief_news(self, url: str, max_links: int = 1) -> CrawlResult:
        """AIChief Featured ë‰´ìŠ¤ íŠ¹í™” í¬ë¡¤ë§"""
        result = CrawlResult(url)
        page = None
        
        try:
            page = await self.context.new_page()
            print("ğŸ¤– AIChief Featured ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(3000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # AIChief Featured ë‰´ìŠ¤ ì°¾ê¸° (ìµœì‹  ìš°ì„ )
            featured_news = await page.evaluate(r'''() => {
                const posts = [];
                console.log('ğŸ” AIChief Featured ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘...');
                
                // Featured ì„¹ì…˜ì˜ ë‰´ìŠ¤ íƒ€ê²ŸíŒ… (ë” êµ¬ì²´ì )
                const featuredSelectors = [
                    // Featured ë¼ë²¨ì´ ìˆëŠ” ê¸°ì‚¬ë“¤ (ìµœìš°ì„ )
                    'div:has(span:contains("Featured")) a',
                    '.featured a',
                    '[class*="featured"] a',
                    // ìƒë‹¨ ë©”ì¸ ë‰´ìŠ¤ ì˜ì—­
                    '.news-grid a',
                    '.post-grid a', 
                    '.article-grid a',
                    // ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë“¤
                    'article a',
                    '.news-item a',
                    '.post a',
                    'h2 a',
                    'h3 a'
                ];
                
                const processedHrefs = new Set();
                
                for (const selector of featuredSelectors) {
                    try {
                        const links = document.querySelectorAll(selector);
                        console.log(`AIChief ì„ íƒì "${selector}": ${links.length}ê°œ ë§í¬ ë°œê²¬`);
                        
                        for (const link of links) {
                            const href = link.href;
                            const title = (link.textContent || '').trim();
                            
                            // AIChief ë‰´ìŠ¤ ë§í¬ ê²€ì¦
                            if (href && title && title.length > 15 && 
                                !processedHrefs.has(href) &&
                                !href.includes('#') &&
                                !href.includes('mailto:') &&
                                !href.includes('javascript:') &&
                                !href.includes('aichief.com/submit') &&
                                !href.includes('aichief.com/contact')) {
                                
                                processedHrefs.add(href);
                                
                                // Featured ì—¬ë¶€ í™•ì¸ (ë” ì •êµí•˜ê²Œ)
                                let isFeatured = false;
                                let featuredContainer = link.closest('div, article, section');
                                while (featuredContainer && !isFeatured) {
                                    const containerHtml = featuredContainer.outerHTML.toLowerCase();
                                    const containerText = featuredContainer.textContent.toLowerCase();
                                    
                                    if (containerHtml.includes('featured') || 
                                        containerText.includes('featured') ||
                                        featuredContainer.querySelector('span:contains("Featured")')) {
                                        isFeatured = true;
                                    }
                                    featuredContainer = featuredContainer.parentElement;
                                }
                                
                                // ë‚ ì§œ ì¶”ì¶œ (AIChief íŠ¹í™”)
                                let dateStr = '';
                                const container = link.closest('div, article, .news-item, .post');
                                if (container) {
                                    // AIChief ë‚ ì§œ í˜•ì‹: "May 23, 2025 6:30 AM"
                                    const dateSelectors = ['time', '.date', '.published', '.timestamp', 'span[class*="date"]'];
                                    for (const dateSelector of dateSelectors) {
                                        const dateEl = container.querySelector(dateSelector);
                                        if (dateEl) {
                                            dateStr = dateEl.getAttribute('datetime') || 
                                                     dateEl.textContent || '';
                                            if (dateStr.trim()) break;
                                        }
                                    }
                                    
                                    // ë‚ ì§œ íŒ¨í„´ ë§¤ì¹­ (ë°±ì—…)
                                    if (!dateStr) {
                                        const textContent = container.textContent;
                                        const dateMatch = textContent.match(/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4}/i);
                                        if (dateMatch) {
                                            dateStr = dateMatch[0];
                                        }
                                    }
                                }
                                
                                // í˜ì´ì§€ ìƒë‹¨ ìœ„ì¹˜ ê³„ì‚°
                                const rect = link.getBoundingClientRect();
                                const topPosition = rect.top + window.scrollY;
                                
                                posts.push({
                                    url: href,
                                    title: title,
                                    selector: selector,
                                    date: dateStr.trim(),
                                    isFeatured: isFeatured,
                                    topPosition: topPosition
                                });
                                
                                console.log(`AIChief ë‰´ìŠ¤: ${title.substring(0, 50)}... (Featured: ${isFeatured}, ë‚ ì§œ: ${dateStr})`);
                            }
                        }
                    } catch (e) {
                        console.warn(`AIChief ì„ íƒì ì˜¤ë¥˜ ${selector}:`, e);
                    }
                }
                
                // ìš°ì„ ìˆœìœ„ ì •ë ¬: Featured > ë‚ ì§œ > ìƒë‹¨ ìœ„ì¹˜
                posts.sort((a, b) => {
                    // 1ìˆœìœ„: Featured ì—¬ë¶€
                    if (a.isFeatured && !b.isFeatured) return -1;
                    if (!a.isFeatured && b.isFeatured) return 1;
                    
                    // 2ìˆœìœ„: ë‚ ì§œ (ìµœì‹ ìˆœ)
                    if (a.date && b.date) {
                        try {
                            return new Date(b.date) - new Date(a.date);
                        } catch (e) {
                            // ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ ë¹„êµ
                            return b.date.localeCompare(a.date);
                        }
                    }
                    if (a.date && !b.date) return -1;
                    if (!a.date && b.date) return 1;
                    
                    // 3ìˆœìœ„: í˜ì´ì§€ ìƒë‹¨ ìœ„ì¹˜
                    return a.topPosition - b.topPosition;
                });
                
                console.log('AIChief ì •ë ¬ëœ ìƒìœ„ ë‰´ìŠ¤:');
                posts.slice(0, 5).forEach((post, i) => {
                    console.log(`${i + 1}. ${post.title.substring(0, 60)}... (Featured: ${post.isFeatured}, ë‚ ì§œ: ${post.date || 'N/A'})`);
                });
                
                return posts.slice(0, 8);  // ìƒìœ„ 8ê°œ í›„ë³´
            }''');
            
            print(f"ğŸ” ë°œê²¬ëœ AIChief ë‰´ìŠ¤: {len(featured_news)}ê°œ")
            
            # ìƒìœ„ í›„ë³´ë“¤ ì¶œë ¥
            for i, news in enumerate(featured_news[:5]):
                featured_mark = "â­" if news['isFeatured'] else "  "
                print(f"   {featured_mark} {i+1}. {news['title'][:60]}... ({news.get('date', 'No date')})")
            
            processed_count = 0
            for i, news in enumerate(featured_news):
                if processed_count >= max_links:
                    break
                    
                try:
                    print(f"ğŸ“„ AIChief ë‰´ìŠ¤ {processed_count+1}: {news['title'][:50]}...")
                    
                    news_page = await self.context.new_page()
                    await news_page.goto(news['url'], wait_until='domcontentloaded', timeout=45000)
                    await news_page.wait_for_timeout(2000)
                    
                    # AIChief ë‰´ìŠ¤ ë‚´ìš© ì¶”ì¶œ
                    content_data = await news_page.evaluate(r'''() => {
                        const article = document.querySelector('article') ||
                                       document.querySelector('main') ||
                                       document.querySelector('.content') ||
                                       document.querySelector('.post-content') ||
                                       document.querySelector('#content');
                        
                        if (!article) return { text: '', date: '', title: '' };
                        
                        const title = document.querySelector('h1')?.textContent?.trim() ||
                                     document.querySelector('.title')?.textContent?.trim() ||
                                     document.title.split('|')[0].trim();
                        
                        // AIChief ë‚ ì§œ ì¶”ì¶œ
                        const dateSelectors = [
                            'time[datetime]',
                            'meta[property="article:published_time"]',
                            '.date',
                            '.published',
                            '.timestamp',
                            'span[class*="date"]'
                        ];
                        
                        let date = '';
                        for (const selector of dateSelectors) {
                            const el = document.querySelector(selector);
                            if (el) {
                                date = el.getAttribute('datetime') || 
                                       el.getAttribute('content') || 
                                       el.textContent;
                                if (date && date.trim()) {
                                    date = date.trim();
                                    break;
                                }
                            }
                        }
                        
                        return {
                            text: article.textContent.trim(),
                            title: title,
                            date: date
                        };
                    }''');
                    
                    if content_data['text'] and len(content_data['text']) > 200:
                        # ë‚ ì§œ ìœ íš¨ì„± í™•ì¸
                        final_date = content_data['date'] or news.get('date', '')
                        parsed_date = parse_date(final_date) if final_date else ''
                        
                        # 48ì‹œê°„ ë‚´ ê¸°ì‚¬ì¸ì§€ í™•ì¸
                        if parsed_date:
                            is_recent = is_relevant_date(parsed_date, None, 48)
                            if not is_recent:
                                print(f"â° 48ì‹œê°„ ì´ˆê³¼: {news['title'][:50]}... ({parsed_date})")
                                await news_page.close()
                                continue
                        
                        story = Story()
                        story.headline = content_data['title'] or news['title']
                        story.link = news['url']
                        story.date_posted = parsed_date
                        story.fullContent = content_data['text']
                        story.imageUrls = []
                        story.videoUrls = []
                        story.source = "aichief.com"
                        
                        result.stories.append(story)
                        processed_count += 1
                        print(f"âœ… AIChief ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ: '{story.headline[:50]}...' (ì¶œì²˜: {story.source})")
                    else:
                        print(f"âš ï¸ ì½˜í…ì¸  ë¶€ì¡±: {len(content_data.get('text', ''))}ì")
                    
                    await news_page.close()
                    
                except Exception as e:
                    print(f"âŒ AIChief ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    if 'news_page' in locals():
                        await news_page.close()
            
            await page.close()
            print(f"ğŸ‰ AIChief í¬ë¡¤ë§ ì™„ë£Œ: {len(result.stories)}ê°œ ë‰´ìŠ¤")
            
        except Exception as e:
            result.error = str(e)
            print(f"âŒ AIChief í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            if page:
                await page.close()
        
        return result

async def main():
    """í–¥ìƒëœ ë©”ì¸ í•¨ìˆ˜ - ë” ë‚˜ì€ ì—ëŸ¬ ì²˜ë¦¬ì™€ ì§„í–‰ ìƒí™© í‘œì‹œ"""
    try:
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        
        parser = argparse.ArgumentParser(
            description="í–¥ìƒëœ ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ì›¹ í¬ë¡¤ëŸ¬",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python dynamic_crawl.py --sources_config '[{"identifier": "https://techcrunch.com/", "maxItems": 3}]'
  python dynamic_crawl.py --sources_config '[{"url": "https://medium.com/", "max_items": 2}, {"url": "https://reddit.com/r/technology", "max_items": 1}]' --content_focus "AI technology"
            """
        )
        
        parser.add_argument(
            "--sources_config",
            type=str,
            required=True,
            help='JSON ì„¤ì •: [{"identifier": "URL", "maxItems": ìˆ˜}, ...] ë˜ëŠ” [{"url": "URL", "max_items": ìˆ˜}, ...]'
        )
        parser.add_argument(
            "--output", 
            type=str, 
            default="crawl_results.json", 
            help="ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: crawl_results.json)"
        )
        parser.add_argument(
            "--llm_provider", 
            type=str, 
            default="openai", 
            choices=["openai", "together", "deepseek"], 
            help="LLM ì œê³µì ì„ íƒ"
        )
        parser.add_argument(
            "--target_date", 
            type=str, 
            default=None, 
            help="ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹). ì´ ë‚ ì§œ ì´í›„ì˜ ì½˜í…ì¸ ë§Œ ìˆ˜ì§‘"
        )
        parser.add_argument(
            "--content_focus", 
            type=str, 
            default=None, 
            help="ê´€ì‹¬ í‚¤ì›Œë“œ (ì˜ˆ: 'AI technology machine learning')"
        )
        parser.add_argument(
            "--timeframe_hours", 
            type=int, 
            default=48, 
            help="ìˆ˜ì§‘ ëŒ€ìƒ ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 48ì‹œê°„)"
        )
        parser.add_argument(
            "--headless", 
            action="store_true", 
            default=True,
            help="ë¸Œë¼ìš°ì € í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (ê¸°ë³¸ê°’: True)"
        )
        parser.add_argument(
            "--show-browser", 
            action="store_true", 
            help="ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ (ë””ë²„ê¹…ìš©)"
        )

        args = parser.parse_args()
        
        # ì†ŒìŠ¤ ì„¤ì • íŒŒì‹±
        try:
            sources_config_list = json.loads(args.sources_config)
            if not isinstance(sources_config_list, list):
                sources_config_list = [sources_config_list]
            print(f"âœ… ì†ŒìŠ¤ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {len(sources_config_list)}ê°œ ì‚¬ì´íŠ¸")
        except json.JSONDecodeError as e:
            print(f"âŒ JSON ì„¤ì • íŒŒì‹± ì˜¤ë¥˜: {e}")
            print("ì˜¬ë°”ë¥¸ í˜•ì‹: '[{\"identifier\": \"https://example.com\", \"maxItems\": 3}]'")
            sys.exit(1)

        # ë¸Œë¼ìš°ì € ëª¨ë“œ ì„¤ì •
        headless_mode = args.headless and not args.show_browser
        if args.show_browser:
            print("ğŸ–¥ï¸ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ ëª¨ë“œ")
        
        print(f"""
ğŸš€ í¬ë¡¤ë§ ì‹œì‘
{'='*60}
ğŸ“Š ì„¤ì • ì •ë³´:
   â€¢ ëŒ€ìƒ ì‚¬ì´íŠ¸: {len(sources_config_list)}ê°œ
   â€¢ ì‹œê°„ ë²”ìœ„: {args.timeframe_hours}ì‹œê°„
   â€¢ ëŒ€ìƒ ë‚ ì§œ: {args.target_date or 'ì œí•œ ì—†ìŒ'}
   â€¢ ê´€ì‹¬ í‚¤ì›Œë“œ: {args.content_focus or 'ì œí•œ ì—†ìŒ'}
   â€¢ ê²°ê³¼ íŒŒì¼: {args.output}
   â€¢ ë¸Œë¼ìš°ì € ëª¨ë“œ: {'í—¤ë“œë¦¬ìŠ¤' if headless_mode else 'í‘œì‹œ'}
{'='*60}
        """)

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = DynamicCrawler(
            llm_provider=args.llm_provider,
            headless=headless_mode,
            target_date=args.target_date
        )
        
        try:
            await crawler.initialize()
            print("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            sys.exit(1)
        
        all_results = []
        successful_sites = 0
        total_stories = 0
        
        # ê° ì‚¬ì´íŠ¸ ì²˜ë¦¬
        for i, source_config_item in enumerate(sources_config_list, 1):
            url = source_config_item.get("identifier") or source_config_item.get("url")
            max_links_for_source = source_config_item.get("max_items", source_config_item.get("maxItems", 1))
            
            if not url:
                print(f"âš ï¸ ì‚¬ì´íŠ¸ {i}: URL ëˆ„ë½, ê±´ë„ˆëœ€ - {source_config_item}")
                continue

            print(f"""
ğŸ“ ì‚¬ì´íŠ¸ {i}/{len(sources_config_list)} ì²˜ë¦¬ ì¤‘
   â€¢ URL: {url}
   â€¢ ìµœëŒ€ ë§í¬: {max_links_for_source}ê°œ
   â€¢ ì‹œê°„ ë²”ìœ„: {args.timeframe_hours}ì‹œê°„
""")
            
            try:
                result_obj = await crawler.crawl_url_targeted(
                    url=url,
                    content_focus=args.content_focus,
                    max_links=max_links_for_source,
                    timeframe_hours_for_filter=args.timeframe_hours 
                )
                
                all_results.append(result_obj.to_dict())
                
                # ê²°ê³¼ ìš”ì•½
                stories_count = len(result_obj.stories)
                if stories_count > 0:
                    successful_sites += 1
                    total_stories += stories_count
                    print(f"âœ… ì‚¬ì´íŠ¸ {i} ì™„ë£Œ: {stories_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    
                    # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ê°„ë‹¨ ìš”ì•½ (ì†ŒìŠ¤ ì •ë³´ í¬í•¨)
                    for j, story in enumerate(result_obj.stories, 1):
                        print(f"   {j}. {story.headline[:50]}... (ì¶œì²˜: {story.source})")
                else:
                    print(f"âš ï¸ ì‚¬ì´íŠ¸ {i} ì™„ë£Œ: ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì—†ìŒ")
                    if result_obj.error:
                        print(f"   ì˜¤ë¥˜: {result_obj.error}")
                
            except Exception as e:
                print(f"âŒ ì‚¬ì´íŠ¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ë¹ˆ ê²°ê³¼ ì¶”ê°€
                all_results.append(CrawlResult(url).to_dict())
            
            # ë‹¤ìŒ ì‚¬ì´íŠ¸ ì²˜ë¦¬ ì „ ëŒ€ê¸° (ë§ˆì§€ë§‰ ì‚¬ì´íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°)
            if i < len(sources_config_list):
                delay = min(5, max(2, len(sources_config_list) - i))  # ë™ì  ëŒ€ê¸° ì‹œê°„
                print(f"â³ {delay}ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ ì‚¬ì´íŠ¸ ì²˜ë¦¬...")
                await asyncio.sleep(delay)
        
        await crawler.close()
        print("âœ… í¬ë¡¤ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")
        
        # ê²°ê³¼ ì €ì¥
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.output}")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            # ë°±ì—… íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ ì‹œë„
            backup_file = f"crawl_results_backup_{int(time.time())}.json"
            try:
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(all_results, f, ensure_ascii=False, indent=2)
                print(f"âœ… ë°±ì—… íŒŒì¼ë¡œ ì €ì¥: {backup_file}")
            except Exception as backup_error:
                print(f"âŒ ë°±ì—… ì €ì¥ë„ ì‹¤íŒ¨: {backup_error}")
        
        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"""
ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!
{'='*60}
ğŸ“Š ìµœì¢… ê²°ê³¼:
   â€¢ ì²˜ë¦¬ëœ ì‚¬ì´íŠ¸: {len(sources_config_list)}ê°œ
   â€¢ ì„±ê³µí•œ ì‚¬ì´íŠ¸: {successful_sites}ê°œ
   â€¢ ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {total_stories}ê°œ
   â€¢ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ
   â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {duration/len(sources_config_list):.1f}ì´ˆ/ì‚¬ì´íŠ¸
   â€¢ ê²°ê³¼ íŒŒì¼: {args.output}
{'='*60}

ğŸ“ˆ ì‚¬ì´íŠ¸ë³„ ìƒì„¸ ê²°ê³¼:""")
        
        for i, (source_config, result) in enumerate(zip(sources_config_list, all_results), 1):
            url = source_config.get("identifier") or source_config.get("url", "Unknown")
            stories_count = len(result.get('stories', []))
            site_info = result.get('site_info', {})
            platform = site_info.get('platform', 'unknown')
            cms = site_info.get('cms', 'unknown')
            
            status = "âœ…" if stories_count > 0 else ("âš ï¸" if result.get('error') else "âŒ")
            print(f"   {status} ì‚¬ì´íŠ¸ {i}: {stories_count}ê°œ ê¸°ì‚¬")
            print(f"      URL: {url}")
            print(f"      ìœ í˜•: {platform}/{cms}")
            if result.get('error'):
                print(f"      ì˜¤ë¥˜: {result['error']}")
        
        if total_stories == 0:
            print("\nâš ï¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
            print("   ğŸ“Š ì§„ë‹¨ ë° ê°œì„  ë°©ì•ˆ:")
            print("   1. URL ì ‘ê·¼ì„±:")
            print("      â€¢ URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
            print("      â€¢ ì‚¬ì´íŠ¸ê°€ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸ (ë°©í™”ë²½, ì§€ì—­ ì°¨ë‹¨ ë“±)")
            print("      â€¢ HTTPS/HTTP í”„ë¡œí† ì½œ í™•ì¸")
            print("   2. ì‹œê°„ ì„¤ì •:")
            print("      â€¢ --timeframe_hoursë¥¼ 72 ë˜ëŠ” 168(ì¼ì£¼ì¼)ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”")
            print("      â€¢ --target_date ì„¤ì •ì„ ì œê±°í•˜ê±°ë‚˜ ì¡°ì •í•´ë³´ì„¸ìš”")
            print("   3. ì½˜í…ì¸  í•„í„°:")
            print("      â€¢ --content_focus í‚¤ì›Œë“œë¥¼ ë‹¨ìˆœí™”í•˜ê±°ë‚˜ ì œê±°í•´ë³´ì„¸ìš”")
            print("      â€¢ 'AI' ëŒ€ì‹  'tech' ë˜ëŠ” 'technology' ë“± ë” ë„“ì€ í‚¤ì›Œë“œ ì‚¬ìš©")
            print("   4. ì‚¬ì´íŠ¸ë³„ íŠ¹ì„±:")
            print("      â€¢ ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” JavaScriptê°€ ë§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            print("      â€¢ --show-browser ì˜µì…˜ìœ¼ë¡œ ì‹¤ì œ í˜ì´ì§€ ë¡œë”© ìƒíƒœ í™•ì¸")
            print("   5. ë””ë²„ê¹…:")
            print("      â€¢ ê°œë³„ ì‚¬ì´íŠ¸ë¥¼ í•˜ë‚˜ì”© í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”")
            print("      â€¢ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•˜ì—¬ êµ¬ì¡° í™•ì¸")
            print("   6. ëŒ€ì•ˆ ì„¤ì •:")
            print("      â€¢ max_itemsë¥¼ ë” ëŠ˜ë ¤ë³´ì„¸ìš” (3-5ê°œ)")
            print("      â€¢ ë‹¤ë¥¸ ë¹„ìŠ·í•œ ì‚¬ì´íŠ¸ë“¤ë„ ì¶”ê°€í•´ë³´ì„¸ìš”")
            
            # ì‹¤í–‰ëœ ì„¤ì • ìš”ì•½
            print(f"\nğŸ“‹ ì‹¤í–‰ëœ ì„¤ì • ìš”ì•½:")
            print(f"   â€¢ ì²˜ë¦¬ ì‚¬ì´íŠ¸: {len(sources_config_list)}ê°œ")
            for i, config in enumerate(sources_config_list, 1):
                url = config.get("identifier") or config.get("url", "Unknown")
                max_items = config.get("max_items", config.get("maxItems", 1))
                print(f"     {i}. {url} (ìµœëŒ€ {max_items}ê°œ)")
            print(f"   â€¢ ì‹œê°„ ë²”ìœ„: {args.timeframe_hours}ì‹œê°„")
            print(f"   â€¢ ëŒ€ìƒ ë‚ ì§œ: {args.target_date or 'ì œí•œ ì—†ìŒ'}")
            print(f"   â€¢ ê´€ì‹¬ í‚¤ì›Œë“œ: {args.content_focus or 'ì œí•œ ì—†ìŒ'}")
            
            # ì¶”ì²œ ëª…ë ¹ì–´
            print(f"\nğŸ”§ ì¶”ì²œ ì¬ì‹¤í–‰ ëª…ë ¹ì–´:")
            print(f"   # ë” ê´€ëŒ€í•œ ì‹œê°„ ì„¤ì •:")
            print(f"   python dynamic_crawl.py --sources_config '{args.sources_config}' --timeframe_hours 168")
            print(f"   # í‚¤ì›Œë“œ í•„í„° ì œê±°:")
            print(f"   python dynamic_crawl.py --sources_config '{args.sources_config}' --timeframe_hours 72")
            print(f"   # ë¸Œë¼ìš°ì € í‘œì‹œ ëª¨ë“œë¡œ ë””ë²„ê¹…:")
            print(f"   python dynamic_crawl.py --sources_config '{args.sources_config}' --show-browser")
        
        else:
            print(f"\nğŸ‰ ìˆ˜ì§‘ ì„±ê³µ! ì´ {total_stories}ê°œì˜ ìµœì‹  AI/ê¸°ìˆ  ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ“° ë‹¤ìŒ ë‹¨ê³„: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½ ìƒì„±")
        
        return all_results
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print("\nğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 